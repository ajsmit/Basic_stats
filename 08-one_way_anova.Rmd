# One-way ANOVA


```{r prelim-opts, echo=FALSE}
knitr::opts_chunk$set(
  comment = "R>", 
  warning = FALSE, 
  message = FALSE
)
```

```{r, echo=FALSE}
library(tidyverse)
library(ggpubr)
```

Whole big books have been written about Analysis of Variance (ANOVA). Although there are many ANOVA experimental designs available, biologists are taught to pay special attention to the design of experiments, and generally make sure that the experiments are fully factorial (in the case of two-way or higher ANOVAs) and balanced. For this reason we will focus in this Introductory Statistics course on one-way and factorial ANOVAs only.

As t-tests, ANOVAs require that some assumptions are met:

* Normally distributed data
* Independence of data
* In our case, we will encourage also that the data are balanced

If some of the above assumptions are violated, then your course of action is either to transform the data (if non-normal) or to use a generalised linear model (also when non-normal), or to use a linear mixed model (when the assumption on non-independence cannot be guaranteed). We will get to some of these methods in later chapters. Linked to the above, ANOVAs are also sensitive to the presence of outliers (see our earlier discussion about the mean and how it differs from the median), so we need to ensure that outliers are not present (they can be removed, and there are many ways of finding them and eliminating them). If outliers are an important feature of the data, then a non-parametric test can be used, or some other test that works well with extreme values can be applied. 

Rather than talking about t-tests and ANOVAs as separate things, let us acknowledge that they are similar ways of asking the same question. That question being, are the means of these two or more things we want to compare different, or the same? At this stage it is important to note that the independent variable is categorical (i.e. a factor denoting two or more different treatments or sampling conditions) and that the dependent variable is continuous. You may perhaps be more familiar with this question when it is presented as a set of hypotheses.

> H0: Group A is not different from group B.
>
> H1: Group A is different from group B.

This is a scientific question in the simplest sense. Often, for basic inquiries such as that posed above, we need to see if one group differs significantly from another. The way in which we accomplish this is by looking at the mean and variance within a set of data compared against another similar set. In order to do so appropriately however we need to first assume that both sets of data are normally distributed, and that the variance found within each set of data is similar. These are the two primary assumptions we learned about in Chapter 6, and if they are met then we may use parametric tests. We will learn in Chapter 9 what we can do if these assumptions are not meant and we cannot adequately transform our data, meaning we will need to use non-parametric tests.

## t-test

A t-test is used when wants to compare two different sample sets against one another. This is also known as a two-factor or two level test. When one wants to compare multiple, more than two, sample sets against one another an ANOVA is required (see below). In order to illustrate how to perform a t-test in R we are going to once again use the `chicks` data, but only Diet 1 and 2 from day 21.

```{r}
# First grab the data
chicks <- as_tibble(ChickWeight)

# Then subset out only the sample sets to be compared
chicks_sub <- chicks %>% 
  filter(Diet %in% c(1, 2), Time == 21)
```

Once we have filtered our data we may now perform the t-test. Traditionally this would be performed with `t.test()`, but recent developments in R have made any testing for the comparison of means more convenient by wrapping everything up into the one single function `compare_means()`. We may use only this one single function for many of the tests we will perform in this chapter as well as Chapter 9. To use `compare_means()` for a t-test we must simply specify this in the `method` argument, as seen below:

```{r}
compare_means(weight ~ Diet, data = chicks_sub, method = "t.test")
```

As one may recall from the Intro R Workshop, whenever we want to give a formula to a function in R, we use the `~`. The formula used above, `weight ~ Diet`, reads in plain English as "weight as a function of diet". This is perhaps easier to understand as "Y as a function of X". This means that we are assuming whatever is to the left of the `~` is the dependant variable, and whatever is to the right is the independent variable. We then tell `compare_means()` to run a t-test on our `chicks_sub` dataframe and it does the rest. We see in the output above that this function gives us a rather tidy read-out of the information we require to test a potential hypothesis. Let's take a moment to look through the help file for this function and see what all of this means. Did the Diet 1 and 2 produce significantly fatter birds?

## ANOVA

### Single factor

In the previous section we learned how to calculate the difference between two sample sets. But what if, as is often the case, we want to compare three or more? Again, the `chicks` data provide an excellent example of how to do this. The base R function for an ANOVA is `aov()`, but we will rather continue to use `compare_means()`. To look for significant differences between all four diets on the last day of sampling we use this one line of code:

```{r}
compare_means(weight ~ Diet, data = filter(chicks, Time == 21), method = "anova")
```

When we now look at all four diets, rather than only 1 and 2, do they produce significantly different chicken weights after 21 days? If this seems to easy to be true, it's because we aren't quite done yet. The next step one must take is to run a Tukey test on the results of the ANOVA by wrapping `tukeyHSD()` around `aov()`:

```{r}
TukeyHSD(aov(weight ~ Diet, data = filter(chicks, Time == 21)))
```

The output of `tukeyHSD()` shows us that pairwise comparisons of all of the groups we are comparing. Let's look at the help file for this function to better understand what the output means. Which of the groups are significantly different from one another? Why does the ANOVA return a significant result, but the Tukey test shows that not all of the groups are significantly different from one another?

### Multiple factors

What if we have multiple grouping variables, and not just one? To run an ANOVA on multiple factors we will need to use `aov()` rather than `compare_means()`. To specify the different factors we put them in our formula and separate them with a `+`:

```{r}
summary(aov(weight ~ Diet + as.factor(Time), data = filter(chicks, Time %in% c(20, 21))))
```

What question are we asking with the above line of code? What is the answer? Also, why did we wrap `Time` in `as.factor()`?

It is also possible to look at what the effect is between grouping variables, and not just within the individual grouping variables. To do this we replace the `+` in our formula with `*`:

```{r}
summary(aov(weight ~ Diet * as.factor(Time), data = filter(chicks, Time %in% c(20, 21))))
```

How do these results differ from the previous set?

One may also run a post-hoc Tukey test on these results the same as for a single factor ANOVA:

```{r}
TukeyHSD(aov(weight ~ Diet * as.factor(Time), data = filter(chicks, Time %in% c(20, 21))))
```

Jikes! That's a massive amount of results. What does all of this mean, and why is it so verbose?

## Exercises

### Exercise 1

Write out the hypotheses that we tested for in this chapter and answer them based on the results we produced in class.
