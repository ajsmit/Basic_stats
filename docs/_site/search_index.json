[
["index.html", "Basic Statistics Preface", " Basic Statistics A primer in basic statistics for BCB (Hons) 2018 AJ Smit and Robert Schlegel 2018-04-05 Preface This is a workshop about the practice of the basic statistics used by biologists, and not about the theory and mathematical underpinnings of the methods used. Each of the Chapters will cover a basic kind of statistical approach, and the main classes of data it applies to. Since much insight and understanding can be gained from visualising our data, we will also explore the main types of graphical summaries that best accompany the statistical methodologies. It is our intention to demonstrate how we go about analysing our data. "],
["prerequisites.html", "Prerequisites", " Prerequisites A prerequisite for this course is a basic proficiency in using R (R Core Team 2017). The necessary experience will have been gained from completing the Intro R Workshop: Data Manipulation, Analysis, and Graphing Workshop that was part of your BCB Core Honours module (i.e. Biostatistics). You will also need a laptop with R and RStudio installed as per the instructions provided in that workshop. If you do not have a personal laptop, most computers in the 5th floor lab will be correctly set up for this purpose. References "],
["introduction.html", "Chapter 1 Introduction 1.1 Venue, date and time 1.2 Course outline 1.3 About this Workshop 1.4 This is biology: why more R coding? 1.5 Installing R and RStudio 1.6 Resources 1.7 Style and code conventions 1.8 Assessment and teaching philosophy 1.9 About this document", " Chapter 1 Introduction “A scientist worthy of a lab coat should be able to make original discoveries while wearing a clown suit, or give a lecture in a high squeaky voice from inhaling helium. It is written nowhere in the math of probability theory that one may have no fun.” — Eliezer Yudkowsky “Prediction is very difficult, especially about the future.” –– Niels Bohr 1.1 Venue, date and time Basic Statistics is the second half of the BSc (Hons) Biostats core module, and will run from 9 April to 20 April 2018. This workshop will take place each day from 9:00–16:00. There will be an assignment due about six weeks after the end of this module, and it will provide the other half of the marks for the Biostats module. More on the assignment later. 1.2 Course outline Introduction (this chapter) Types of data Descriptive statistics: Measures of location and dispersion Representing data graphically The Normal distribution and probability determination using the normal curve T-tests (one-sample, two-sample and paired) Confidence intervals One-way ANOVA Multivariate ANOVA Repeated measures ANOVA (or rather LME?) Testing data for normality and transforming data Non-parametric tests—Mann-Whitney, Kruskal-Wallis (or rather GLM?) Chi square tests Correlation Linear regression The course content can broadly be classified into two parts: Descriptive Statistics and Inferential Statistics. Descriptive statistics and their associated statistical and graphical data summaries will be covered in Chapters 3 and 4. In Chapter 5 we will introduce the concepts of data distributions, knowledge of which is required to select the most appropriate inferential statistical methods. Chapters 6-15 are about inferential statistics. Inferential tests allow us to evaluate hypotheses within a framework of probabalistic theory, which helps us infer the nature of a ‘population’ based on a smaller represenative set of samples. In partiucular, we can infer whether the property under scrutiny (arrived at by means of a designed experiment or a directed sampling programme) occured as a result of deterministic influences, or whether it is as a result of chance. 1.3 About this Workshop The aim of this five-day introductory workshop is to guide you through the outline given above. 1.4 This is biology: why more R coding? Please refer to the Intro R Workshop: Data Manipulation, Analysis and Graphing for why we feel strongly that you use R (R Core Team 2017) for the analyses that we will perform here. All of the reasons provided there are valid here too, but one reason perhaps more so than others—R and RStudio promote the principles of reproducible research, and in fact make it very easy to implement. We will focus on some of these principles throughout the workshop, and the assignments will require that you submit a fully functional working script, complete with all the notes, memos, examples, data, executable code, and output that will result from completing the course material. What other options are there for analysing the kinds of data that we will encounter in biological research? Software packages like the ones you may be familiar with, such as Statistica and SPSS, are often used to perform many of these analyses. They are rather limited with regards to the full scope of modern statistical methods in use by biologists today, but many people still use these kinds of software as they provide the basic kinds analyses that still form the staple of the biological and medical sciences. For the many reasons provided above, we prefer to use R as the engine within which to do our biological data analysis. R is used by academic statisticians the world over, and it is therefore an excellent choice for our purpose here. 1.5 Installing R and RStudio We assume that you already have R installed on your computer, as all of you will have already completed the the Intro R Workshop. If you need a refresher, please refer to Intro R Workshop: Data Manipulation, Analysis and Graphing for the installation instructions. 1.6 Resources 1.6.1 Required reading 1.6.2 General resources about R 1.7 Style and code conventions Early on, develop the habit of unambiguous and consistent style and formatting when writing your code, or anything else for that matter. Pay attention to detail and be pedantic. This will benefit your scientific writing in general. Although many R commands rely on precisely formatted statements (code blocks), style can nevertheless to some extent have a personal flavour to it. The key is consistency. In this book we use certain conventions to improve readability. We also use a consistent set of conventions to refer to code, and in particular to typed commands and package names. Package names are shown in a bold font over a grey box, e.g. tidyr. Functions are shown in normal font followed by parentheses and also over a grey box , e.g. plot(), or summary(). Other R objects, such as data, function arguments or variable names are again in normal font over a grey box, but without parentheses, e.g. x and apples. Sometimes we might directly specify the package that contains the function by using two colons, e.g. dplyr::filter(). Commands entered onto the R command line (console) and the output that is returned will be shown in a code block, which is a light grey background with code font. The commands entered start at the beginning of a line and the output it produces is preceded by R&gt;, like so: rnorm(n = 10, mean = 0, sd = 13) R&gt; [1] -0.8759925 0.7343694 -16.7771184 6.6990968 -4.8447596 R&gt; [6] 6.2763795 -2.6443716 -13.2744824 -7.7641231 -21.4471054 Consult these resources for more about R code style : Google’s R style guide The tidyverse style guide Hadley Wickham’s advanced R style guide We may also insert maths expressions within the text, like this \\(f(k) = {n \\choose k} p^{k} (1-p)^{n-k}\\) or on their own, like this: \\[f(k) = {n \\choose k} p^{k} (1-p)^{n-k}\\] 1.8 Assessment and teaching philosophy Grades will be based on performance of two take home exams, and an individual project and homework problem sets. The exams and individual project will represent 30% of the grade. The homework problem sets will make up the remaining 10%. In cases where students are borderline between lower and higher grades, a high level of participation in the class discussions and class in general will win the day for the higher grade. Homework problems are essential to understanding of the materials. Although the homework comprises only 10% of the final grade, performance on the exams is usually correlated with effort on the homework problems. Whereas plagiarism will not be tolerated, students ARE encouraged to work together to learn from one another (especially those from the same IVN site) and solve problems in a collaborative and collegial way (aside from the take home exam). 1.9 About this document This document, which as available as an HTML file that’s viewable on a web browser of your choice (anything will do, but we discourage using Internet Explorer) and as a PDF (accessible from the link at the top of any of the website’s pages) that may be printed, was prepared by the software tools available to R via RStudio. We use the package called bookdown that may be accessed and read about here to produce this documentation. The entire source code to reproduce this book is available from my GitHub repo. knitr::include_graphics(&quot;figures/bookdown_hex_logo.png&quot;) Figure 1.1: Bookdown hex. You will notice that this repository uses GitHub, and you are advised to set up your own repository for R scripts and all your data. We will touch on GitHub and the principles of reproducible research later, and GitHub forms a core ingredient of such a workflow. The R session information when compiling this book is shown below: sessionInfo() R&gt; R version 3.4.4 (2018-03-15) R&gt; Platform: x86_64-apple-darwin15.6.0 (64-bit) R&gt; Running under: macOS High Sierra 10.13.3 R&gt; R&gt; Matrix products: default R&gt; BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib R&gt; LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib R&gt; R&gt; locale: R&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 R&gt; R&gt; attached base packages: R&gt; [1] stats graphics grDevices utils datasets base R&gt; R&gt; loaded via a namespace (and not attached): R&gt; [1] Rcpp_0.12.16 bookdown_0.7 png_0.1-7 digest_0.6.15 R&gt; [5] rprojroot_1.3-2 backports_1.1.2 magrittr_1.5 evaluate_0.10.1 R&gt; [9] highr_0.6 stringi_1.1.7 rmarkdown_1.9 tools_3.4.4 R&gt; [13] stringr_1.3.0 xfun_0.1 yaml_2.1.18 compiler_3.4.4 R&gt; [17] htmltools_0.3.6 knitr_1.20 methods_3.4.4 References "],
["types-of-data.html", "Chapter 2 Types of data 2.1 Data classes 2.2 Viewing our data", " Chapter 2 Types of data “The plural of anecdote is not data.” — Roger Brinner In this chapter we will, firstly, look at the different kinds of biological and environmental data that are typically encountered by most biologists. The data seen here are not an exhaustive list of all the various types of data out there, but it should represent the bulk of our needs. After we have become familiar with the different kinds of data, we will look at summaries of these data, which is generally required as the starting point for our analyses. After summarising the data in tables and so forth, we may want to produce graphical summaries to see broad patterns and trends; visual data representations, which complement the tabulated data, will be covered in a later chapter (Chapter 4). Both of these approaches form the basis of ‘exploratory data analysis.’ 2.1 Data classes In biology we will encounter many kinds of data, and depending on which kind, the type of statistical analysis will be decided. 2.1.1 Numerical data Numerical data are quantitative in nature. They represent things that can be objectively counted or measured. 2.1.1.1 Nominal (discrete) data Integer data (discrete numbers or whole numbers), such as counts. For example, family A may have 3 children and family B may have 1 child, neither may have 2.3 children. Integer data usually answer the question, “how many?” In R integer data are called int or &lt;int&gt;. 2.1.1.2 Continuous data These usually represent measured ‘things,’ such as something’s heat content (temperature, measured in degrees Celsius) or distance (measured in metres or similar), etc. They can be rational numbers including integers and fractions, but typically they have an infinite number of ‘steps’ that depends on rounding (they can even be rounded to whole integers) or considerations such as measurement precision and accuracy. Often, continuous data have upper and lower bounds that depend on the characteristics of the phenomenon being studied or the measurement being taken. In R, continuous data are denoted num or &lt;dbl&gt;. The kinds of summaries that lend themselves to continuous data are: Frequency distributions Relative frequency distributions Cumulative frequency distributions Bar graphs Box plots Scatter plots 2.1.1.3 Dates Dates are a special class of continuous data, and there are many different representations of the date classes. This is a complex group of data, and we will not cover much of it in this course. 2.1.2 Qualitative data Qualitative data may be well-defined categories or they may be subjective, and generally include descriptive words for classes (e.g. mineral, animal , plant) or rankings (e.g. good, better, best). 2.1.2.1 Categorical data Because there are categories, the number of members belonging to each of the categories can be counted. For example, there are three red flowers, 66 purple flowers, and 13 yellow flowers. The categories cannot be ranked relative to each other; in the example just provided, for instance, no value judgement can be assigned to the different colours. It is not better to be red than it is to be purple. There are just fewer red flowers than purple ones. Contrast this to another kind of categorical data called ‘ordinal data’ (see next). This class of data in an R dataframe (or in a ‘tibble’) is indicated by Factor or &lt;fctr&gt;. The kinds of summaries that lend themselves to categorical data are: Frequency distributions Relative frequency distributions Bar graphs Pie graphs (!!!) Category statistics 2.1.2.2 Ordinal data This is a type of categorical data where the classes are ordered (a synonym is “ranked”), typically from low to high (or vice versa), but where the magnitude between the ordered classes cannot be precisely measured or quantified. In other words, the difference between them is somewhat subjective (i.e. it is qualitative rather than quantitative). These data are on an ordinal scale. The data may be entered as descriptive character strings (i.e. as words), or they may have been translated to an ordered vector of integers; for example, “1” for terrible, “2” for so-so, “3” for average, “4” for good and “5” for brilliant. Irrespective of how the data are present in the dataframe, computationally (for some calculations) they are treated as an ordered sequence of integers, but they are simultaneously treated as categories (say, where the number of responses that report “so-so” can be counted). Ordinal data usually answer questions such as, “how many categories can the phenomenon be divided into, and how does each category rank with respect to the others?” Columns containing this kind of data are named Ord.factor or &lt;ord&gt;. 2.1.3 Binary data Right or wrong? True or false? Accept or reject? Black or white? Positive or negative? Good or bad? You get the idea… In other words, these are observations or responses that can take only one of two mutually exclusive outcomes. In R these are treated as ‘Logical’ data that take the values of TRUE or FALSE (note the case). In R, and computing generally, logical data are often denoted with 1 for TRUE and 0 for FALSE. This class of data is indicated by logi or &lt;lgl&gt;. 2.1.4 Character values As the name implies, these are not numbers. Rather, they are human words that have found their way into R for one reason or another. In biology we most commonly encounter character values when we have a list of things, such as sites or species. These values will often be used as categorical or ordinal data. 2.1.5 Missing values Unfortunately, one of the most reliable aspects of any biological dataset is that it will contain some missing data. But how can something contain missing data? One could be forgiven for assuming that if the data are missing, then they obviously aren’t contained in the dataset. TO better understand this concept we must think back to the principles of tidy data. Every observation must be in a row, and every column in that row must contain a value. The combination of multiple observations then makes up our matrix of data. Because data are therefore presented in a two-dimensional format, any missing values from an observation will need to have an empty place-holder to ensure the consistency of the matrix. These are waht we are referring to when we speak of “missing values”. In R these appear as a NA in a dataframe and are slighlty lighter than the other values. These data are indicated in the Environment as NA and if a column contains only missing values it will be denoted as &lt;NA&gt;. 2.1.6 Complex numbers “And if you gaze long enough into an abyss, the abyss will gaze back into you.” — Friedrich Nietzsche In an attempt to allow the shreds of our sanity to remain stitched together we will end here with data types. But be warned, ye who enter, that below countless rocks, and around a legion of corners, lay in wait a myriad of complex data types. We will encounter many of these at the end of this course when we encounter modeling, but by then we will have learned a few techniques that will prepare us for the encounter. 2.2 Viewing our data There are many ways of finding broad views of our data in R. The first few functions that we will look at were designed to simply scrutinise the contents of the tibbles, which is the ‘tidyverse’ name for the general ‘container’ that holds our data in the software’s environment (i.e. in a block of the computer’s memory dedicated to the R software). Whatever data are in R’s environment will be seen in the ‘Environment’ tab in the top right of RStudio’s four panes. 2.2.1 From the Environment pane The first way to see what’s in the tibble is not really a function at all, but a convenient (and lazy) way of quickly seeing a few basic things about our data. Let us look at the ChickWeight data. Load it like so (you’ll remember from the Intro R Workshop): # loads the tidyverse functions; it contains the &#39;as_tibble()&#39; function library(tidyverse) # the &#39;ChickWeight&#39; data are built into R; # here we assign it as a tibble to an object named &#39;chicks&#39; chicks &lt;- as_tibble(ChickWeight) In the Environment pane, the object named chicks will now appear under the panel named Data. To the left of it is a small white arrow in a blue circular background. By default the arrow points to the right. Clicking on it causes it to point down, which denotes that the data contained within the tibble have become expanded. The names of the columns (more correctly called ‘variables’) can now be seen. There you can see the variables weight, Time, Chick and Diet. The class of data they represent can be seen too: there’s continuous data of class num, a variable of Ord.factor, and a categorical variable of class Factor. Beneath these there’s a lot of attributes that denote some meta-data, which you may safely ignore for now. (#fig:chicks_1)What is in the Chicks data? (#fig:chicks_2)This is what is in the Chicks data 2.2.2 head() and tail() The head() and tail() functions simply display top and bottom portions of the tibble, and you may add the n argument and an integer to request that only a certain number of rows are returned; by default the top or bottom six rows are displayed. There are various bits of additional information printed out. The display will change somewhat if there are many more variables than that which can comfortably fit within the width of the output window (typically the Console). The same kinds of information as was returned with the Environment pane expansion arrow are displayed, but the data class is now accompanied by an angle bracket (i.e. &lt;...&gt;) notation. For example, num in the Environment pane and &lt;dbl&gt; as per the head() or tail() methods are exactly the same: both denote continuous (or ‘double precision’) data. head(chicks) R&gt; # A tibble: 6 x 4 R&gt; weight Time Chick Diet R&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;fct&gt; R&gt; 1 42. 0. 1 1 R&gt; 2 51. 2. 1 1 R&gt; 3 59. 4. 1 1 R&gt; 4 64. 6. 1 1 R&gt; 5 76. 8. 1 1 R&gt; 6 93. 10. 1 1 tail(chicks, n = 2) R&gt; # A tibble: 2 x 4 R&gt; weight Time Chick Diet R&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;fct&gt; R&gt; 1 264. 20. 50 4 R&gt; 2 264. 21. 50 4 As an alternative to head(), you may also simply type the name of the object (here chicks) in the Console (or write it in the Source Editor if it is necessary to retain the function for future use) and the top portion of the tibble will be displayed, again trimmed to account for the width of the display. 2.2.3 colnames() This function simply returns a listing of the variable (column) names. colnames(chicks) R&gt; [1] &quot;weight&quot; &quot;Time&quot; &quot;Chick&quot; &quot;Diet&quot; There is an equivalent function called rownames() that may be used to show the names of rows in your tibble, if these are present. Row names are generally discouraged, and we will refrain from using them here. 2.2.4 summary() The next way to see the contents of the tibble is to apply the summary() function. Here we see something else. Some descriptive statistics that describe properties of the full set of data are now visible. These summary statistics condense each of the variables into numbers that describe some properties of the data within each column. You will already know the concepts of the ‘minimum,’ ‘median,’ ‘mean,’ and ‘maximum.’ These are displayed here. summary(chicks) R&gt; weight Time Chick Diet R&gt; Min. : 35.0 Min. : 0.00 13 : 12 1:220 R&gt; 1st Qu.: 63.0 1st Qu.: 4.00 9 : 12 2:120 R&gt; Median :103.0 Median :10.00 20 : 12 3:120 R&gt; Mean :121.8 Mean :10.72 10 : 12 4:118 R&gt; 3rd Qu.:163.8 3rd Qu.:16.00 17 : 12 R&gt; Max. :373.0 Max. :21.00 19 : 12 R&gt; (Other):506 This will serve well as an introduction to the next chapter, which is about descriptive statistics. What are they, and how do we calculate them? "],
["descriptive-statistics-a-focus-on-measures-of-central-tendency-and-dispersion.html", "Chapter 3 Descriptive statistics: A focus on measures of central tendency and dispersion 3.1 Samples and populations 3.2 Measures of central tendency 3.3 Measures of variation and spread 3.4 Missing values 3.5 Descriptive statistics by group 3.6 Exercises", " Chapter 3 Descriptive statistics: A focus on measures of central tendency and dispersion “I think it is much more interesting to live with uncertainty than to live with answers that might be wrong.” –– Richard Feynman In this Chapter we will focus on basic descriptions of the data, and these initial forrays are built around measures of the central tendency of the data (the mean, median, mode) and the dispersion and variability of the data (standard deviations and their ilk). The materials covered in this and the next two chapters concern a broad discussion that will aid us in understanding our data better prior to analysing it, and before we can draw inference from it. In this work flow it emerges that descriptive statistics generally precede inferential statistics. Let us now turn to some of the most commonly used descriptive statistics, and learn about how to calculate them. 3.1 Samples and populations This is a simple toy example. In real life, however, our data will be available in a tibble (initially perhaps captured in MS Excel before importing it as a .csv file into R, where the tibble is created). To see how this can be done more realistically using actual data, let us turn to the ChickenWeight data, which, as before, we place in the object chicks. Recall the pipe operator (%&gt;%, pronounced ‘then’) that we introduced in the Intro R Workshop—we will use that here, throughout. Let us calculate the sample size: To determine the sample size we can use the length() or n() functions; the latter is for use within dplyr’s summarise() method, and it is applied without writing anything inside of the (), like this: # first load the tidyverse packages that has the pipe operator, %&gt;% library(tidyverse) chicks &lt;- as_tibble(ChickWeight) # how many weights are available across all Diets and Times? chicks %&gt;% summarise(length = n()) R&gt; # A tibble: 1 x 1 R&gt; length R&gt; &lt;int&gt; R&gt; 1 578 # the same as length(chicks$weight) R&gt; [1] 578 3.2 Measures of central tendency Measures of central tendency. Statistic Function Package Mean mean() base Median median() base Central moment moment() e1071 Skewness skewness() e1071 Kurtosis kurtosis() e1071 The measures of central tendency are also sometimes called ‘location’ statistics. We have already seen summaries of the mean and the median when we called to summary() function on the chicks data in Chapter 2. Here we shall show you how they can be calculated using some built-in R functions. 3.2.1 The mean The sample mean is the arithmetic average of the data, and it is calculated by summing all the data and dividing it by the sample size, n. The mean, \\(\\bar{x}\\), is calculated thus: \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i} = \\frac{x_{1} + x_{2} + \\cdots + x_{n}}{n}\\] where \\(x_{1} + x_{2} + \\cdots + x_{n}\\) are the observations and \\(n\\) is the number of observations in the sample. In R one can quickly apply the mean() function to some data. Let us create a vector of arbitrary numbers using the ‘combine’ function, c(), and then apply the function for the mean: # combine a series of numbers into a vector; # hint: use this function in the exercises that we will require from you # later on... dat1 &lt;- c(23, 45, 23, 66, 13) mean(dat1) R&gt; [1] 34 Below, we use another tidyverse package, dplyr and its summarise() function, whose purpose it is to summarise the entire column into one summary statistic, in this case the mean: chicks %&gt;% summarise(mean_wt = mean(weight)) R&gt; # A tibble: 1 x 1 R&gt; mean_wt R&gt; &lt;dbl&gt; R&gt; 1 122. We can achieve the same using the more traditional syntax, which in some instances may be slightly less verbose, but less user-friendly, especially when multiple summary statistics are required (we shall later on how we can summarise a vector of data into multiple statistics). The traditional syntax is: # the &#39;$&#39; operator is used to denote a variable inside of the tibble mean(chicks$weight) R&gt; [1] 121.8183 Question: How would you manually calculate the mean mass for the chicks? Do it now! Notice above how the two approaches display the result differently: in the first instance, using summarise(), the answer is rounded to zero decimal places; in the second, it is displayed (here) at full precision. The precision of the answer that you require depends on the context of your study, so make sure that you use the appropriate number of significant digits. Using the summarise() approach again, here is how you can adjust the number of decimal places of the answer: # the value printed in the HTML/PDF versions is incorrect; # check in the console for correct output chicks %&gt;% summarise(mean_wt = round(mean(weight), 1)) R&gt; # A tibble: 1 x 1 R&gt; mean_wt R&gt; &lt;dbl&gt; R&gt; 1 122. Question: What happens when there are missing values (NA)? Consult the help file for the mean() function, discuss amongst yourselves, and then provide a demonstration to the class of how you would handle missing values. Hint: use the c() function to capture a series of data that you can then use to demonstrate your understanding. At this point it might be useful to point out that the mean (or any function for that matter, even one that does not yet exist) can be programatically calculated. Let us demonstrate the principle by reproducing the mean function from the constituent parts: chicks %&gt;% summarise(mean_wt = sum(weight) / n()) R&gt; # A tibble: 1 x 1 R&gt; mean_wt R&gt; &lt;dbl&gt; R&gt; 1 122. The mean is quite sensitive to the presence of outliers or extreme values in the data, and it is advised that its use be reserved for normally distributed data from which the extremes/outliers have been removed. When extreme values are indeed part of our data and not simply ‘noise,’ then we have to resort to a different measure of central tendency: the median. 3.2.2 The median The median can be calculated by ‘hand’ (if you have a small enough amount of data) by arranging all the numbers in sequence from low to high, and then finding the middle value. If there are five numbers, say 5, 2, 6, 13, 1, then you would arrange them from low to high, i.e. 1, 2, 5, 6, 13. The middle number is 5. This is the median. But there is no middle if we have an even number of values. What now? Take this example sequence of six integers (they may also be floating point numbers), which has already been ordered for your pleasure: 1, 2, 5, 6, 9, 13. Find the middle two numbers (i.e. 5, 6) and take the mean. It is 5.5. That is the median. Let us find the median for the weights of the chickens in the ChickWeight data: chicks %&gt;% summarise(med_wt = median(weight)) R&gt; # A tibble: 1 x 1 R&gt; med_wt R&gt; &lt;dbl&gt; R&gt; 1 103. The median is therefore the value that separates the lower half of the sample data from the upper half. In normally distributed continuous data the median is equal to the mean. Comparable concepts to the median are the 1st and 3rd quartiles, which, respectively, separate the first quarter of the data from the last quarter — see later. The advantage of the median over the mean is that it is unaffected (i.e. not skewed) by extreme values or outliers, and it gives an idea of the typical value of the sample. The median is also used to provide a robust description of non-parametric data (see Chapter 4 for a discussion on normal data and other data distributions). 3.2.3 Central moment 3.2.4 Skewness 3.2.5 Kurtosis 3.3 Measures of variation and spread Since the mean or median does not tell us everything there is to know about data, we will also have to determine some statistics that inform us about the variation (or spread) around the central/mean value. Measures of variation and spread. Statistic Function Variance var() Standard deviation sd() Minimum min() Maximum max() Range range() Quantile quantile() Covariance cov() Correlation cor() 3.3.1 The variance and standard deviation The variance and standard deviation are examples of interval estimates. The sample variance, \\(S^{2}\\), may be calculated according to the following formula: \\[S^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}\\] This reads: “the sum of the squared differences from the mean, divided by the sample size minus 1.” To get the standard deviation, \\(S\\), we take the square root of the variance, i.e. \\(S = \\sqrt{S^{2}}\\). No need to plug these equations into MS Excel. Let us quickly calculate \\(S\\) in R. Again, we use the chicks data: chicks %&gt;% summarise(sd_wt = sd(weight)) R&gt; # A tibble: 1 x 1 R&gt; sd_wt R&gt; &lt;dbl&gt; R&gt; 1 71.1 The interpretation of the concepts of mean and median are fairly straight forward and intuitive. Not so for the measures of variance. What does \\(S\\) represent? Firstly, the unit of measurement of \\(S\\) is the same as that of \\(\\bar{x}\\) (but the variance doesn’t share this characteristic). If temperature is measured in °C, then \\(S\\) also takes a unit of °C. Since \\(S\\) measures the dispersion around the mean, we write it as \\(\\bar{x} \\pm S\\) (note that often the mean and standard deviation are written with the letters mu and sigma, respectively; i.e. \\(\\mu \\pm \\sigma\\)). The smaller \\(S\\) the closer the sample data are to \\(\\bar{x}\\), and the larger the value is the further away they will spread out from \\(\\bar{x}\\). So, it tells us about the proportion of observations above and below \\(\\bar{x}\\). But what proportion? We invoke the the 68-95-99.7 rule: ~68% of the population (as represented by a random sample of \\(n\\) observations taken from the population) falls within 1\\(S\\) of \\(\\bar{x}\\) (i.e. ~34% below \\(\\bar{x}\\) and ~34% above \\(\\bar{x}\\)); ~95% of the population falls within 2\\(S\\); and ~99.7% falls within 3\\(S\\). Figure 1.1: The proportions of data representation by the standard deviation. Credit: Wikipedia Like the mean, \\(S\\) is affected by extreme values and outliers, so before we attach \\(S\\) as a summary statistic to describe some data, we need to ensure that the data are in fact normally distributed. We will talk about how to do this in Chapter 6, where we will go over the numerous ways to check the assumption of normality. When the data are found to be non-normal, we need to find appropriate ways to express the spread of the data. Enter the quartiles. 3.3.2 Quantiles A more forgiving approach (forgiving of the extremes, often called ‘robust’) is to divide the distribution of ordered data into quarters, and find the points below which 25% (0.25, the first quartile), 50% (0.50, the median) and 75% (0.75, the third quartile) of the data are distributed. These are called quartiles (for ‘quarter;’ not to be confused with quantile, which is a more general form of the function that can be used to divide the distribution into any arbitrary proportion from 0 to 1). In R we use the quantile() function to provide the quartiles; we demonstrate two approaches: quantile(chicks$weight) R&gt; 0% 25% 50% 75% 100% R&gt; 35.00 63.00 103.00 163.75 373.00 chicks %&gt;% summarise(min_wt = min(weight), qrt1_wt = quantile(weight, p = 0.25), med_wt = median(weight), qrt3_wt = median(weight, p = 0.75), max_wt = max(weight)) R&gt; # A tibble: 1 x 5 R&gt; min_wt qrt1_wt med_wt qrt3_wt max_wt R&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; R&gt; 1 35. 63. 103. 103. 373. # note median(weight) is the same as quantile(weight, p = 0.5) # in the summarise() implementation, above Question: What is different about the quantile() function that caused us to specify the calculation in the way in which we have done so above? You will have to consult the help file, read it, understand it, think about it, and experiment with the ideas. Take 15 minutes to figure it out and report back to the class. 3.3.3 The minimum, maximum and range A description of the extent of the data can also be provided by the functions min(), max() and range(). These statistics apply to data of any distribution, and not only to normal data. This if often the first place you want to start when looking at the data for the first time. We’ve seen above how to use min() and max(), so below we will quickly look at how to use range() in both the base R and tidy methods: range(chicks$weight) R&gt; [1] 35 373 chicks %&gt;% summarise(lower_wt = range(weight)[1], upper_wt = range(weight)[2]) R&gt; # A tibble: 1 x 2 R&gt; lower_wt upper_wt R&gt; &lt;dbl&gt; &lt;dbl&gt; R&gt; 1 35. 373. Note that range() actually gives us the minimum and maximum values, and not the difference between them. To find the range value properly we must be a bit more clever: range(chicks$weight)[2] - range(chicks$weight)[1] R&gt; [1] 338 chicks %&gt;% summarise(range_wt = range(weight)[2] - range(weight)[1]) R&gt; # A tibble: 1 x 1 R&gt; range_wt R&gt; &lt;dbl&gt; R&gt; 1 338. 3.3.4 Covariance 3.3.5 Correlation The correlation coefficient of two matched (paired) variables is equal to their covariance divided by the product of their individual standard deviations. It is a normalised measurement of how linearly related the two variables are. Graphical displays of correlations are provided by scatter plots as can be seen in Section X. 3.4 Missing values As mentioned in Chapter 2, missing data are pervaise in the biological sciences. Happily for us, R is designed to handle these data easily. It is important to note here explicitly that all of the basic functions in R will by default NOT ignore missing data. This has been done so as to prevent the user from accidentally forgetting about the missing data and potentially making errors in later stages in an analysis. Therefore, we must explicitly tell R when we want it to ommit missing values from a calculation. Let’s create a small vector of data to demonstrate this: dat1 &lt;- c(NA, 12, 76, 34, 23) # Without telling R to ommit missing data mean(dat1) R&gt; [1] NA # Ommitting the missing data mean(dat1, na.rm = TRUE) R&gt; [1] 36.25 Note that this argument, na.rm = TRUE may be used in all of the functions we have seen thus far in this chapter. 3.5 Descriptive statistics by group Above we have revised the basic kinds of summary statistics, and how to calculate them. This is nice. But it can be more useful. The real reason why we might want to see the descriptive statistics is to facilitate comparisons between groups. In the chicks data we calculated the mean (etc.) for all the chickens, over all the diet groups to which they had been assigned (there are four factors, i.e. Diets 1 to 4), and over the entire duration of the experiment (the experiment lasted 21 days). It would be more useful to see what the weights are of the chickens in each of the four groups at the end of the experiment — we can compare means (± SD) and medians (± interquartile ranges, etc.), for instance. You’ll notice now how the measures of central tendency is being combined with the measures of variability/range. Further, we can augment this statistical summary with many kinds of graphical summaries, which will be far more revealing of differences (if any) amongst groups. We will revise how to produce the group statistics and show a range of graphical displays. 3.5.1 Groupwise summary statistics At this point you need to refer to Chapter 10 (Tidy data) and Chapter 11 (Tidier data) in the Intro R Workshop to remind yourself about in what format the data need to be before we can efficiently work with it. A hint: one observation in a row, and one variable per column. From this point, it is trivial to do the various data descriptions, visualisations, and analyses. Thankfully, the chicks data are already in this format. So, what are the summary statistics for the chickens for each diet group at day 21? grp_stat &lt;- chicks %&gt;% filter(Time == 21) %&gt;% group_by(Diet, Time) %&gt;% summarise(mean_wt = round(mean(weight, na.rm = TRUE), 2), med_wt = median(weight, na.rm = TRUE), sd_wt = round(sd(weight, na.rm = TRUE), 2), sum_wt = sum(weight), min_wt = min(weight), qrt1_wt = quantile(weight, p = 0.25), med_wt = median(weight), qrt3_wt = median(weight, p = 0.75), max_wt = max(weight), n_wt = n()) grp_stat R&gt; # A tibble: 4 x 11 R&gt; # Groups: Diet [?] R&gt; Diet Time mean_wt med_wt sd_wt sum_wt min_wt qrt1_wt qrt3_wt max_wt R&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; R&gt; 1 1 21. 178. 166. 58.7 2844. 96. 138. 166. 305. R&gt; 2 2 21. 215. 212. 78.1 2147. 74. 169. 212. 331. R&gt; 3 3 21. 270. 281. 71.6 2703. 147. 229. 281. 373. R&gt; 4 4 21. 239. 237. 43.4 2147. 196. 204. 237. 322. R&gt; # ... with 1 more variable: n_wt &lt;int&gt; 3.5.2 Displays of group summaries There are several kinds of graphical displays for your data. We will show some which are able to display the spread of the raw data, the mean or median, as well as the appropriate accompanying indicators of variation around the mean or median. library(ggpubr) # needed for arranging multi-panel plots plt1 &lt;- chicks %&gt;% filter(Time == 21) %&gt;% ggplot(aes(x = Diet, y = weight)) + geom_point(data = grp_stat, aes(x = Diet, y = mean_wt), col = &quot;black&quot;, fill = &quot;red&quot;, shape = 23, size = 3) + geom_jitter(width = 0.05) + # geom_point() if jitter not required labs(y = &quot;Chicken mass (g)&quot;) + theme_pubr() plt2 &lt;- ggplot(data = grp_stat, aes(x = Diet, y = mean_wt)) + geom_bar(position = position_dodge(), stat = &quot;identity&quot;, col = NA, fill = &quot;salmon&quot;) + geom_errorbar(aes(ymin = mean_wt - sd_wt, ymax = mean_wt + sd_wt), width = .2) + labs(y = &quot;Chicken mass (g)&quot;) + theme_pubr() # position_dodge() places bars side-by-side # stat = &quot;identity&quot; prevents the default count from being plotted plt3 &lt;- chicks %&gt;% filter(Time == 21) %&gt;% ggplot(aes(x = Diet, y = weight)) + geom_boxplot(fill = &quot;salmon&quot;) + geom_jitter(width = 0.05, fill = &quot;white&quot;, col = &quot;blue&quot;, shape = 21) + labs(y = &quot;Chicken mass (g)&quot;) + theme_pubr() plt4 &lt;- chicks %&gt;% filter(Time %in% c(10, 21)) %&gt;% ggplot(aes(x = Diet, y = weight, fill = as.factor(Time))) + geom_boxplot() + geom_jitter(shape = 21, width = 0.1) + labs(y = &quot;Chicken mass (g)&quot;, fill = &quot;Time&quot;) + theme_pubr() ggarrange(plt1, plt2, plt3, plt4, ncol = 2, nrow = 2, labels = &quot;AUTO&quot;) Figure 3.1: A) Scatterplot of the mean and raw chicken mass values. B) Bar graph of the chicken mass values, showing ‘whiskers’ indicating ±1 SD. C) Box and whisker plot of the chicken mass data. Please see the help file for geom_boxplot() for what the graph components mean. 3.6 Exercises 3.6.1 Exercise 1 Notice how the data summary for chicken weights contained within wt_summary is very similar to the summary returned for weight when we apply summary(chicks). Please use the summarise() approach and construct a data summary with exactly the same summary statistics for weight as that which summary() returns. "],
["graphical-data-displays.html", "Chapter 4 Graphical data displays 4.1 Frequency distributions 4.2 Bar graphs 4.3 Pie graphs (!!!) 4.4 Category statistics", " Chapter 4 Graphical data displays library(tidyverse) library(ggpubr) 4.1 Frequency distributions 4.1.1 Qualitative data Qualitative data that describe group representivity to various categories are best described with frequency distribution histograms (I interchangably use histograms, frequency histograms, and frequency distribution histograms). Histograms apply to categorical data. Although it can also be presented numerically in tabular form, one may also create a bar or pie graph of the number of occurences in a collection of non-overlapping classes or categories. Both the data and graphical displays will be demonstrated here. The first case of a frequency distribution histogram is one that shows the raw counts per each of the categories that are represented in the data. The count within each of the categories (represented by a bar graph called a histogram) sums to the sample size, \\(n\\). In the second case, we may want to report that data as proportions. Here we show the frequency proportion in a collection of non-overlapping categories. For example, we have a sample size of 12 (\\(n=12\\)). In this sample, two are coloured blue, six red, and five purple. The relative proportions are \\(2/12=0.1666667\\) blue, \\(6/12=0.5\\) red, and \\(5/12=0.4166667\\) purple. The important thing to note here is that the relative proportions sum to 1, i.e. \\(0.1666667+0.5+0.4166667=1\\). These data may be presented as a table or as a graph. 4.1.2 Continuous data 4.1.2.1 Frequency distributions (histograms) As with discrete data, we have a choice of absolute and relative frequency histograms. Since the purpose of frequency histograms is to count the number of times something takes place or occurs within a category, what do we do when we are faced with continuous data where no categories are available? We can create our own categories, called bins. See the Old Faithful data, for example. The eruptions last between 1.6 and 5.1 minutes. So, we create intervals of time spanning these times, and within each count the number of times an event lasts as long as denoted by the intervals. Here we might choose intervals of 1-2 minutes, 2-3 minutes, 3-4 minutes, 4-5 minutes, and 5-6 minutes. The ggplot2 geom_histogram() function automatically creates the bins, but we may specify our own. It is best to explain these principles by example (see Figure X A-B). # a normal frequency histogram, with count along y hist1 &lt;- ggplot(data = faithful, aes(x = eruptions)) + geom_histogram(colour = &quot;black&quot;, fill = &quot;salmon&quot;, alpha = 0.6) + labs(title = &quot;Old Faithful data&quot;, subtitle = &quot;A vanilla frequency histogram&quot;, x = &quot;Eruption duration (min)&quot;, y = &quot;Count&quot;) # when the binwidth is 1, the density histogram *is* the relative # frequency histogram hist2 &lt;- ggplot(data = faithful, aes(x = eruptions)) + geom_histogram(aes(y = ..density..), position = &#39;identity&#39;, binwidth = 1, colour = &quot;black&quot;, fill = &quot;salmon&quot;, alpha = 0.6) + labs(title = &quot;Old Faithful data&quot;, subtitle = &quot;Relative frequency histogram&quot;, x = &quot;Eruption duration (min)&quot;, y = &quot;Count&quot;) # if binwidth is something other than 1, the relative frequency in # a histogram is ..density.. * binwidth hist3 &lt;- ggplot(data = faithful, aes(x = eruptions)) + geom_histogram(aes(y = 0.5 * ..density..), position = &#39;identity&#39;, binwidth = 0.5, colour = &quot;black&quot;, fill = &quot;salmon&quot;, alpha = 0.6) + labs(title = &quot;Old Faithful data&quot;, subtitle = &quot;Relative frequency histogram&quot;, x = &quot;Eruption duration (min)&quot;, y = &quot;Count&quot;) ggarrange(hist1, hist2, hist3, ncol = 2, nrow = 2, labels = &quot;AUTO&quot;) Figure 4.1: Examples of histograms for the Old Faithful data. A) A default frequency histogram showing the count of eruption times falling within the specified bins. B) A relative frequency histogram with bins adjusted to a width of 1 minute intervals; here, the sum of counts within each of the four bins is 1. C) Another relative frequency histogram, but with the bins adjusted to each be 0.5 minute increments; again the sum of counts represented by each bin is equal to 1. 4.1.2.2 Cumulative frequency distributions 4.1.2.3 Cumulative relative frequency distributions 4.1.2.4 Scatter plots Relationship between two (matched) continuous variables. 4.1.2.5 Box plots Box plots are sometimes called box-and-whisker plots. These graphs are a a graphical representation of the data based on its quartiles as well as its smallest and largest values. The keen eye can glance the ‘shape’ of the data distribution. 4.2 Bar graphs 4.3 Pie graphs (!!!) 4.4 Category statistics "],
["distributions.html", "Chapter 5 Distributions 5.1 Discrete distributions 5.2 Continuous distributions", " Chapter 5 Distributions Therefore, we must next learn about the different types of data distributions we are likely to encounter in the wild. 5.1 Discrete distributions A discrete random variable has a finite or countable number of possible values. As the name suggests, it models integer data. Below we provide options to generate and visualise data belonging to several classes of discrete distributions. Later (Chapter X) we will learn how to transform these data prior to performing the approapriate statistical analysis. 5.1.1 Bernoulli distribution A Bernoulli random variable, \\(x\\), takes the value 1 with probability \\(p\\) and the value 0 with probability \\(q=1−p\\). It is used to represent data resulting from a single experiment with binary (yes or no; black or white; positive or negative; success or failure; dead or alive;) outcomes, such as a coin toss—there are only two options, heads or tails. Nothing else. Here, \\(p\\) represents the probability of the one outcome and \\(q\\) the probability of the other outcome. The distribution of the possible outcomes, \\(x\\), is given by: \\[ f(x;p)= \\begin{cases} p, &amp;\\text{if}~x=1\\\\ 1-p, &amp;\\text{if}~x=0 \\end{cases} \\] 5.1.2 Binomial distribution A binomial random variable, \\(x\\), is the sum of \\(n\\) independent Bernoulli random variables with parameter \\(p\\). This data distribution results from repeating identical experiments that produce a binary outcome with probability \\(p\\) a specified number of times, and choosing \\(n\\) samples at random. As such, it represents a collection of Bernoulli trials. \\[f(x;n,p)= {n\\choose x}p^{x}(1-p)^{n-x}\\] 5.1.3 Negative binomial distribution A negative binomial random variable, \\(x\\), counts the number of successes in a sequence of independent Bernoulli trials with probability \\(p\\) before \\(r\\) failures occur. This distribution could for example be used to predict the number of heads that result from a series of coin tosses before three tails are observed: \\[f(x;n,r,p)= {x+r-1\\choose x}p^{x}(1-p)^{r}\\] 5.1.4 Geometric distribution A geometric random variable, \\(x\\), represents the number of trials that are required to observe a single success. Each trial is independent and has success probability \\(p\\). As an example, the geometric distribution is useful to model the number of times a die must be tossed in order for a six to be observed. It is given by: \\[f(x;p)=(1-p)^{x}p\\] 5.1.5 Poisson distribution A Poisson random variable, \\(x\\), tallies the number of events occurring in a fixed interval of time or space, given that these events occur with an average rate \\(\\lambda\\). Poisson distributions can be used to model events such as meteor showers and or number of people entering a shopping mall. This equation describes the Poison distribution: \\[f(x;\\lambda)=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}\\] 5.2 Continuous distributions 5.2.1 Uniform distribution 5.2.2 Normal distribution library(plotly) x &lt;- rnorm(100, 5, 1) plot_ly(x = x, type = &quot;histogram&quot;) %&gt;% add_boxplot() Figure 5.1: Boxplot and probability density function of a normal distribution N(0, σ2). Credit: Wikipedia 5.2.3 Student T distribution 5.2.4 Chi-squared distribution 5.2.5 Exponential distribution 5.2.6 F distribution 5.2.7 Gamma distribution 5.2.8 Beta distribution "],
["inferences-about-one-or-two-populations.html", "Chapter 6 Inferences about one or two populations 6.1 One-sample t-tests 6.2 Two-sample t-tests 6.3 Paired t-tests 6.4 Comparison of Two Population Proportions", " Chapter 6 Inferences about one or two populations library(ggpubr) Assumption of t-tests… Normal distribution 6.1 One-sample t-tests prop.test() 6.1.1 One-sided one-sample t-tests 6.1.2 Two-sided one-sample t-tests 6.2 Two-sample t-tests 6.2.1 One-sided two-sample t-tests 6.2.2 Two-sided two-sample t-tests 6.3 Paired t-tests 6.4 Comparison of Two Population Proportions …also prop.test() "],
["confidence-intervals.html", "Chapter 7 Confidence intervals", " Chapter 7 Confidence intervals A confidence interval (CI) tells us within what range we may be certain to find the true mean from which any sample has been taken. To calculate a confidence interval requires three things: # First we set the sample mean sample_mean &lt;- 10 # Then the sample standard deviation sample_sd &lt;- 2 # Then the number of samples sample_n &lt;- 20 Once we know these things we must then use the following formula to calculate th CI: error &lt;- qnorm(0.975)*sample_sd/sqrt(sample_n) With the CI known, we then substract/add it from the sample mean in order to find the upper and lower limits: lower &lt;- sample_mean-error upper &lt;- sample_mean+error lower R&gt; [1] 9.123477 upper R&gt; [1] 10.87652 "],
["one-way-anova.html", "Chapter 8 One-way ANOVA 8.1 t-test 8.2 ANOVA 8.3 Exercises", " Chapter 8 One-way ANOVA ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4 ## ✔ tidyr 0.8.0 ✔ stringr 1.3.0 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ .GlobalEnv::arrange() masks dplyr::arrange() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ dplyr::n() masks .env::n() ## Loading required package: magrittr ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract Whole big books have been written about Analysis of Variance (ANOVA). Although there are many ANOVA experimental designs available, biologists are taught to pay special attention to the design of experiments, and generally make sure that the experiments are fully factorial (in the case of two-way or higher ANOVAs) and balanced. For this reason we will focus in this Introductory Statistics course on one-way and factorial ANOVAs only. As t-tests, ANOVAs require that some assumtions are met: Normally distributed data Independence of data In our case, we will encourage also that the data are balanced If some of the above assumptions are violated, then your course of action is either to transform the data (if non-normal) or to use a generalised linear model (also when non-normal), or to use a linear mixed model (when the assumption on non-independence cannot be garanteed). We will get to some of these methods in later chapters. Linked to the above, ANOVAs are also sensitive to the presence of outliers (see our earlier discussion about the mean and how it differs from the median), so we need to ensure that outliers are not present (they can be removed, and there are many ways of finding them and eliminating them). If outliers are an important feature of the data, then a non-parametric test can be used, or some other test that works well with extreme values can be applied. Rather than talking about t-tests and ANOVAs as separate things, let us acknowledge that they are similar ways of asking the same question. That question being, are the means of these two or more things we want to compare different, or the same? At this stage it is important to note that the independent variable is categorical (i.e. a factor denoting two or more different treatments or sampling conditions) and that the dependent variable is continuous. You may perhaps be more familiar with this question when it is presented as a set of hypotheses. H0: Group A is not different from group B. H1: Group A is different from group B. This is a scientific question in the simplest sense. Often, for basic inquiries such as that posed above, we need to see if one group differs significantly from another. The way in which we accomplish this is by looking at the mean and variance within a set of data compared against another similar set. In order to do so appropriately however we need to first assume that both sets of data are normally distributed, and that the variance found within each set of data is similar. These are the two primary assumptions we learned about in Chapter 6, and if they are met then we may use paramteric tests. We will learn in Chapter 9 what we can do if these assumptions are not meant and we cannot adequately transform our data, meaning we will need to use non-parametric tests. 8.1 t-test A t-test is used when wants to compare two different sample sets against one another. This is also known as a two-factor or two level test. When one wants to compare multiple, more than two, sample sets against one another an ANOVA is required (see below). In order to illustrate how to perform a t-test in R we are going to once again use the chicks data, but only Diet 1 and 2 from day 21. # First grab the data chicks &lt;- as_tibble(ChickWeight) # Then subset out only the sample sets to be compared chicks_sub &lt;- chicks %&gt;% filter(Diet %in% c(1, 2), Time == 21) Once we have filtered our data we may now perform the t-test. Traditionally this would be performed with t.test(), but recent developments in R have made any testing for the comparison of means more convenient by wrapping everything up into the one single function compare_means(). We will need to use only this one signle function for all of the tests we will perform in this chapter as well as Chapter 9. To use compare_means() for a t-test we must simplu specify this in the method argument, as seen below: compare_means(weight ~ Diet, data = chicks_sub, method = &quot;t.test&quot;) R&gt; # A tibble: 1 x 8 R&gt; .y. group1 group2 p p.adj p.format p.signif method R&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; R&gt; 1 weight 1 2 0.218 0.218 0.22 ns T-test As one may recall from the Intro R Workshop, whenever we want to give a formula to a function in R, we use the ~. The formula used above, weight ~ Diet, reads in plain English as “weight as a function of diet”. This is perhaps easier to understand as “Y as a function of X”. This means that we are assuming whatever is to the left of the ~ is the dependant variable, and whatever is to the right is the independent variable. We then tell compare_means() to run a t-test on our chicks_sub dataframe and it does the rest. We see in the output above that this function gives us a rather tidy read-out of the information we require to test a potential hypothesis. Let’s take a moment to look through the help file for this function and see what all of this means. Did the Diet 1 and 2 produce significantly fatter birds? 8.2 ANOVA 8.2.1 Single factor In the previous section we learned how to calculate the difference between two smaple sets. But what if, as is often the case, we want to compare three or more? Again, the chicks data provide an excellent example of how to do this. The base R function for an ANOVA is aov(), but we will rather continue to use compare_means(). To look for significant differences between all four diets on the last day of sampling we use this one line of code: compare_means(weight ~ Diet, data = filter(chicks, Time == 21), method = &quot;anova&quot;) R&gt; # A tibble: 1 x 6 R&gt; .y. p p.adj p.format p.signif method R&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; R&gt; 1 weight 0.00686 0.00686 0.0069 ** Anova When we now look at all four diets, rather than only 1 and 2, do they produce significantly different chicken weights after 21 days? If this seems to easy to be true, it’s because we aren’t quite done yet. The next step one must take is to run a Tukey test on the results of the ANOVA by wrapping tukeyHSD() around aov(): TukeyHSD(aov(weight ~ Diet, data = filter(chicks, Time == 21))) R&gt; Tukey multiple comparisons of means R&gt; 95% family-wise confidence level R&gt; R&gt; Fit: aov(formula = weight ~ Diet, data = filter(chicks, Time == 21)) R&gt; R&gt; $Diet R&gt; diff lwr upr p adj R&gt; 2-1 36.95000 -32.11064 106.01064 0.4868095 R&gt; 3-1 92.55000 23.48936 161.61064 0.0046959 R&gt; 4-1 60.80556 -10.57710 132.18821 0.1192661 R&gt; 3-2 55.60000 -21.01591 132.21591 0.2263918 R&gt; 4-2 23.85556 -54.85981 102.57092 0.8486781 R&gt; 4-3 -31.74444 -110.45981 46.97092 0.7036249 The output of tukeyHSD() shows us tha pairwise comparisons of all of the groups we are comparing. Let’s look at the help file for this function to better understand what the output means. Which of the groups are significantly different from one another? Why does the ANOVA return a significant result, but the Tukey test shows that not all of the groups are significantly different from one another? 8.2.2 Multiple factors What if we have multiple grouping variables, and not just one? To run an ANOVA on multiple factors we will need to use aov() rather than compare_means(). To specify the different factors we put them in our formula and separate them with a +: summary(aov(weight ~ Diet + as.factor(Time), data = filter(chicks, Time %in% c(20, 21)))) R&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) R&gt; Diet 3 113270 37757 10.436 6.33e-06 *** R&gt; as.factor(Time) 1 1488 1488 0.411 0.523 R&gt; Residuals 86 311147 3618 R&gt; --- R&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What question are we asking with the above line of code? Waht is the answer? Also, why did we wrap Time in as.factor()? It is also possible to look at what the effect is between gouping variables, and not just within the individual grouping variables. To do this we replace the + in our formula with *: summary(aov(weight ~ Diet * as.factor(Time), data = filter(chicks, Time %in% c(20, 21)))) R&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) R&gt; Diet 3 113270 37757 10.076 9.88e-06 *** R&gt; as.factor(Time) 1 1488 1488 0.397 0.530 R&gt; Diet:as.factor(Time) 3 117 39 0.010 0.999 R&gt; Residuals 83 311030 3747 R&gt; --- R&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How do these results differ from the previous set? One may also run a post-hoc Tukey test on these results the same as for a single factor ANOVA: TukeyHSD(aov(weight ~ Diet * as.factor(Time), data = filter(chicks, Time %in% c(20, 21)))) R&gt; Tukey multiple comparisons of means R&gt; 95% family-wise confidence level R&gt; R&gt; Fit: aov(formula = weight ~ Diet * as.factor(Time), data = filter(chicks, Time %in% c(20, 21))) R&gt; R&gt; $Diet R&gt; diff lwr upr p adj R&gt; 2-1 36.18030 -9.301330 81.66194 0.1663037 R&gt; 3-1 90.63030 45.148670 136.11194 0.0000075 R&gt; 4-1 62.25253 15.223937 109.28111 0.0045092 R&gt; 3-2 54.45000 3.696023 105.20398 0.0305957 R&gt; 4-2 26.07222 -26.072532 78.21698 0.5586643 R&gt; 4-3 -28.37778 -80.522532 23.76698 0.4863940 R&gt; R&gt; $`as.factor(Time)` R&gt; diff lwr upr p adj R&gt; 21-20 8.088223 -17.44017 33.61661 0.5303164 R&gt; R&gt; $`Diet:as.factor(Time)` R&gt; diff lwr upr p adj R&gt; 2:20-1:20 35.188235 -40.67378 111.050253 0.8347209 R&gt; 3:20-1:20 88.488235 12.62622 164.350253 0.0111136 R&gt; 4:20-1:20 63.477124 -14.99365 141.947897 0.2035951 R&gt; 1:21-1:20 7.338235 -58.96573 73.642198 0.9999703 R&gt; 2:21-1:20 44.288235 -31.57378 120.150253 0.6116081 R&gt; 3:21-1:20 99.888235 24.02622 175.750253 0.0023872 R&gt; 4:21-1:20 68.143791 -10.32698 146.614563 0.1371181 R&gt; 3:20-2:20 53.300000 -31.82987 138.429869 0.5234263 R&gt; 4:20-2:20 28.288889 -59.17374 115.751515 0.9723470 R&gt; 1:21-2:20 -27.850000 -104.58503 48.885027 0.9486212 R&gt; 2:21-2:20 9.100000 -76.02987 94.229869 0.9999766 R&gt; 3:21-2:20 64.700000 -20.42987 149.829869 0.2732059 R&gt; 4:21-2:20 32.955556 -54.50707 120.418182 0.9377007 R&gt; 4:20-3:20 -25.011111 -112.47374 62.451515 0.9862822 R&gt; 1:21-3:20 -81.150000 -157.88503 -4.414973 0.0305283 R&gt; 2:21-3:20 -44.200000 -129.32987 40.929869 0.7402877 R&gt; 3:21-3:20 11.400000 -73.72987 96.529869 0.9998919 R&gt; 4:21-3:20 -20.344444 -107.80707 67.118182 0.9960548 R&gt; 1:21-4:20 -56.138889 -135.45396 23.176184 0.3619622 R&gt; 2:21-4:20 -19.188889 -106.65152 68.273738 0.9972631 R&gt; 3:21-4:20 36.411111 -51.05152 123.873738 0.8984019 R&gt; 4:21-4:20 4.666667 -85.06809 94.401428 0.9999998 R&gt; 2:21-1:21 36.950000 -39.78503 113.685027 0.8067041 R&gt; 3:21-1:21 92.550000 15.81497 169.285027 0.0075185 R&gt; 4:21-1:21 60.805556 -18.50952 140.120628 0.2629945 R&gt; 3:21-2:21 55.600000 -29.52987 140.729869 0.4679025 R&gt; 4:21-2:21 23.855556 -63.60707 111.318182 0.9896157 R&gt; 4:21-3:21 -31.744444 -119.20707 55.718182 0.9486128 Jikes! That’s a massive amount of results. What does all of this mean, and why is it so verbose? 8.3 Exercises 8.3.1 Exercise 1 Write out the hypotheses that we tested for in this chapter and answer them based on the results we produced in class. "],
["deeper-into-anova.html", "Chapter 9 Deeper into ANOVA 9.1 Wilcox rank sum test 9.2 Kruskall-Wallis rank sum test 9.3 Exercises 9.4 Exercise 1", " Chapter 9 Deeper into ANOVA In the previous chapter we learned how to test hypotheses based on the comparions of means between sets of data when we were able to meet our two base assumptions. These parametric tests are preferred over non-parametric tests because they are more robust. However, when we simply aren’t able to meet these assumptions we must not despair. Non-parametric tests are still useful. In this chapter we will learn how to run non-parametirc tests for two sample and multiple sample datasets. To start, let’s load our libraries and chicks data if we have not already. # First activate libraries library(tidyverse) library(ggpubr) # Then load data chicks &lt;- as_tibble(ChickWeight) With our libraries and data loaded, let’s find a day in which at least one of our assumptions are violated. # Then check for failing assumptions chicks %&gt;% filter(Time == 0) %&gt;% group_by(Diet) %&gt;% summarise(norm_wt = as.numeric(shapiro.test(weight)[2]), var_wt = var(weight)) R&gt; # A tibble: 4 x 3 R&gt; Diet norm_wt var_wt R&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; R&gt; 1 1 0.0138 0.989 R&gt; 2 2 0.138 2.23 R&gt; 3 3 0.00527 1.07 R&gt; 4 4 0.0739 1.11 9.1 Wilcox rank sum test The non-parametric version of a t-test is a Wilcox rank sum test. To perform this test in R we may again use compare_means() and specify the test we want: compare_means(weight ~ Diet, data = filter(chicks, Time == 0, Diet %in% c(1, 2)), method = &quot;wilcox.test&quot;) R&gt; # A tibble: 1 x 8 R&gt; .y. group1 group2 p p.adj p.format p.signif method R&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; R&gt; 1 weight 1 2 0.235 0.235 0.23 ns Wilcoxon What do our results show? 9.2 Kruskall-Wallis rank sum test 9.2.1 Single factor The non-parametric version of an ANOVA is a Kruskall-Wallis rank sum test. As you may have by now surmised, this may be done with compare_means() as seen below: compare_means(weight ~ Diet, data = filter(chicks, Time == 0), method = &quot;kruskal.test&quot;) R&gt; # A tibble: 1 x 6 R&gt; .y. p p.adj p.format p.signif method R&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; R&gt; 1 weight 0.475 0.475 0.48 ns Kruskal-Wallis As with the ANOVA, this first step with the Kruskall-Wallis test is not the last. We must again run a post-hoc test on our results. This time we will need to use pgirmess::kruskalmc(), which means we will need to load a new library. library(pgirmess) kruskalmc(weight ~ Diet, data = filter(chicks, Time == 0)) R&gt; Multiple comparison test after Kruskal-Wallis R&gt; p.value: 0.05 R&gt; Comparisons R&gt; obs.dif critical.dif difference R&gt; 1-2 6.95 14.89506 FALSE R&gt; 1-3 6.90 14.89506 FALSE R&gt; 1-4 4.15 14.89506 FALSE R&gt; 2-3 0.05 17.19933 FALSE R&gt; 2-4 2.80 17.19933 FALSE R&gt; 3-4 2.75 17.19933 FALSE Let’s consult the help file for kruskalmc() to understand what this print-out means. 9.2.2 Multiple factors The water becomes murky quickly when one wants to perform mutliple factor non-parametric comparison of means tests. TO that end, we will not cover the few existing methods here. Rather, one should avoid the necessity for these types of tests when designing an experiment. 9.3 Exercises 9.4 Exercise 1 Write out the hypotheses that we tested for in this chapter and answer them based on the results we produced in class. "],
["linear-mixed-models.html", "Chapter 10 Linear mixed models 10.1 Wilcox rank sum test 10.2 Kruskall-Wallis rank sum test 10.3 Exercises 10.4 Exercise 1", " Chapter 10 Linear mixed models In the previous chapter we learned how to test hypotheses based on the comparions of means between sets of data when we were able to meet our two base assumptions. These parametric tests are preferred over non-parametric tests because they are more robust. However, when we simply aren’t able to meet these assumptions we must not despair. Non-parametric tests are still useful. In this chapter we will learn how to run non-parametirc tests for two sample and multiple sample datasets. To start, let’s load our libraries and chicks data if we have not already. # First activate libraries library(tidyverse) library(ggpubr) # Then load data chicks &lt;- as_tibble(ChickWeight) With our libraries and data loaded, let’s find a day in which at least one of our assumptions are violated. # Then check for failing assumptions chicks %&gt;% filter(Time == 0) %&gt;% group_by(Diet) %&gt;% summarise(norm_wt = as.numeric(shapiro.test(weight)[2]), var_wt = var(weight)) R&gt; # A tibble: 4 x 3 R&gt; Diet norm_wt var_wt R&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; R&gt; 1 1 0.0138 0.989 R&gt; 2 2 0.138 2.23 R&gt; 3 3 0.00527 1.07 R&gt; 4 4 0.0739 1.11 10.1 Wilcox rank sum test The non-parametric version of a t-test is a Wilcox rank sum test. To perform this test in R we may again use compare_means() and specify the test we want: compare_means(weight ~ Diet, data = filter(chicks, Time == 0, Diet %in% c(1, 2)), method = &quot;wilcox.test&quot;) R&gt; # A tibble: 1 x 8 R&gt; .y. group1 group2 p p.adj p.format p.signif method R&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; R&gt; 1 weight 1 2 0.235 0.235 0.23 ns Wilcoxon What do our results show? 10.2 Kruskall-Wallis rank sum test 10.2.1 Single factor The non-parametric version of an ANOVA is a Kruskall-Wallis rank sum test. As you may have by now surmised, this may be done with compare_means() as seen below: compare_means(weight ~ Diet, data = filter(chicks, Time == 0), method = &quot;kruskal.test&quot;) R&gt; # A tibble: 1 x 6 R&gt; .y. p p.adj p.format p.signif method R&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; R&gt; 1 weight 0.475 0.475 0.48 ns Kruskal-Wallis As with the ANOVA, this first step with the Kruskall-Wallis test is not the last. We must again run a post-hoc test on our results. This time we will need to use pgirmess::kruskalmc(), which means we will need to load a new library. library(pgirmess) kruskalmc(weight ~ Diet, data = filter(chicks, Time == 0)) R&gt; Multiple comparison test after Kruskal-Wallis R&gt; p.value: 0.05 R&gt; Comparisons R&gt; obs.dif critical.dif difference R&gt; 1-2 6.95 14.89506 FALSE R&gt; 1-3 6.90 14.89506 FALSE R&gt; 1-4 4.15 14.89506 FALSE R&gt; 2-3 0.05 17.19933 FALSE R&gt; 2-4 2.80 17.19933 FALSE R&gt; 3-4 2.75 17.19933 FALSE Let’s consult the help file for kruskalmc() to understand what this print-out means. 10.2.2 Multiple factors The water becomes murky quickly when one wants to perform mutliple factor non-parametric comparison of means tests. TO that end, we will not cover the few existing methods here. Rather, one should avoid the necessity for these types of tests when designing an experiment. 10.3 Exercises 10.4 Exercise 1 Write out the hypotheses that we tested for in this chapter and answer them based on the results we produced in class. "],
["testing-assumptions-or-how-i-learned-to-stop-worrying-and-transform-the-data.html", "Chapter 11 Testing assumptions or: How I learned to stop worrying and transform the data 11.1 The two main assumptions 11.2 Transforming data 11.3 Exercises", " Chapter 11 Testing assumptions or: How I learned to stop worrying and transform the data In this chapter we will learn how to test for the two most common assumptions we will make in the biological sciences. Our tests for these two assumptions fail often with real data. Therefore, we must often identify the way in which our data are distributed so we may better decide how to transform them in an attempt to coerce them into a format that will pass the assumptions. Before we begin, let’s go ahead and activate our packages and load our data. library(tidyverse) chicks &lt;- as_tibble(ChickWeight) 11.1 The two main assumptions 11.1.1 Normality The quickest method of testing the normality of a variable is with the Shapiro-Wilk normality test. This will return two values, a W score and a p-value. FOr the purposes of this course we may safely ignore the W score and focus on the p-value. When p &gt;= 0.05 we may assume that the data are normally distributed. If p &lt; 0.05 then the data are not normally distrubted. Let’s look at all of the chicks data without filtering it: shapiro.test(chicks$weight) R&gt; R&gt; Shapiro-Wilk normality test R&gt; R&gt; data: chicks$weight R&gt; W = 0.90866, p-value &lt; 2.2e-16 Are these data normally distributed? How do we know? Now let’s filter the data based on the different diets for only the weights taken on the last day (21): chicks %&gt;% filter(Time == 21) %&gt;% group_by(Diet) %&gt;% summarise(norm_wt = as.numeric(shapiro.test(weight)[2])) R&gt; # A tibble: 4 x 2 R&gt; Diet norm_wt R&gt; &lt;fct&gt; &lt;dbl&gt; R&gt; 1 1 0.591 R&gt; 2 2 0.949 R&gt; 3 3 0.895 R&gt; 4 4 0.186 How about now? 11.1.2 Homoscedasticity Here we need no fancy test. We must simply calculate the variance of the variables we want to use and see that they are not more than 3 – 4 times greater than one another. chicks %&gt;% filter(Time == 21) %&gt;% group_by(Diet) %&gt;% summarise(var_wt = var(weight)) R&gt; # A tibble: 4 x 2 R&gt; Diet var_wt R&gt; &lt;fct&gt; &lt;dbl&gt; R&gt; 1 1 3446. R&gt; 2 2 6106. R&gt; 3 3 5130. R&gt; 4 4 1879. Well, do these data pass the two main assumptions? 11.2 Transforming data 11.2.1 Log transform 11.2.2 11.3 Exercises 11.3.1 Exercise 1 Find one of the days of measurement where the chicken weights do not pass the assumptions of normality, and another day (not day 21!) in which they do. 11.3.2 Exercise 2 Transform the data so that they may pass the assumptions. "],
["generalised-linear-models.html", "Chapter 12 Generalised linear models 12.1 Sign Test 12.2 Wilcoxon Signed-Rank Test 12.3 Mann-Whitney-Wilcoxon Test 12.4 Kruskal-Wallis Test 12.5 Generalised linear models (GLM)", " Chapter 12 Generalised linear models 12.1 Sign Test 12.2 Wilcoxon Signed-Rank Test 12.3 Mann-Whitney-Wilcoxon Test 12.4 Kruskal-Wallis Test 12.5 Generalised linear models (GLM) "],
["chi-squared.html", "Chapter 13 Chi-squared", " Chapter 13 Chi-squared A chi-squared test is used when one wants to see if there is a realtionship between count data of two or more factors. x &lt;- c(A = 20, B = 15, C = 25) chisq.test(x) R&gt; R&gt; Chi-squared test for given probabilities R&gt; R&gt; data: x R&gt; X-squared = 2.5, df = 2, p-value = 0.2865 "],
["correlations.html", "Chapter 14 Correlations", " Chapter 14 Correlations A chi-squared test is used when one wants to see if there is a realtionship between count data of two or more factors. x &lt;- c(A = 20, B = 15, C = 25) chisq.test(x) R&gt; R&gt; Chi-squared test for given probabilities R&gt; R&gt; data: x R&gt; X-squared = 2.5, df = 2, p-value = 0.2865 "],
["simple-linear-regressions.html", "Chapter 15 Simple linear regressions 15.1 The simple linear regression equation 15.2 Using an additional categorical variable", " Chapter 15 Simple linear regressions ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4 ## ✔ tidyr 0.8.0 ✔ stringr 1.3.0 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ .GlobalEnv::arrange() masks dplyr::arrange() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ dplyr::n() masks .env::n() Regressions test the statistical significance of the dependence of one continuous variable on one or many independent continuous variables. 15.1 The simple linear regression equation The linear regression equation is already known to you. It is: \\[y_{n}=\\beta \\cdot x_{n}+\\alpha+\\epsilon\\] Coefficients are parameters (statistics) that describe two properties of the linear line that best fit a scatter plot between a dependent variable and the independent variable. The dependent variable, \\(y_{n}\\), may also be called the response variable, and the independent variable, \\(x_{n}\\), the predictor. The regression model consists of an intercept term, \\(\\alpha\\), that describes where the fitted line starts and intercepts with the y-axis, and the slope, \\(\\beta\\), of the line. The amount of variation not explained by a linear relationship of \\(y\\) on \\(x\\) is termed the residual variation, or simply the residual or the error term, and in the above equation it is indicated by \\(\\epsilon\\). The parameters \\(\\alpha\\) and \\(\\beta\\) are determined by minimising the sum of squares of the error term, \\(\\epsilon\\). It allows us to predict new fitted values of \\(y\\) based on values of \\(x\\). We will demonstrate the principle behind a simple linear regression by using the built-in dataset faithful. According to the R help file for the data, the dataset describes the “Waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA.” head(faithful) R&gt; eruptions waiting R&gt; 1 3.600 79 R&gt; 2 1.800 54 R&gt; 3 3.333 74 R&gt; 4 2.283 62 R&gt; 5 4.533 85 R&gt; 6 2.883 55 In this dataset there are two columns: the first, eruptions, denotes the duration of the eruption (in minutes), and the second, waiting, is the waiting time (also in minutes) until the next eruptions. Its linear regression model can be expressed as: \\[eruption_{n}=\\beta \\cdot waiting_{n}+\\alpha+\\epsilon\\] Here we fit the model in R. When we perform a linear regression in R, it’ll output the model and the coefficients: eruption.lm = lm(eruptions ~ waiting, data = faithful) summary(eruption.lm) R&gt; R&gt; Call: R&gt; lm(formula = eruptions ~ waiting, data = faithful) R&gt; R&gt; Residuals: R&gt; Min 1Q Median 3Q Max R&gt; -1.29917 -0.37689 0.03508 0.34909 1.19329 R&gt; R&gt; Coefficients: R&gt; Estimate Std. Error t value Pr(&gt;|t|) R&gt; (Intercept) -1.874016 0.160143 -11.70 &lt;2e-16 *** R&gt; waiting 0.075628 0.002219 34.09 &lt;2e-16 *** R&gt; --- R&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 R&gt; R&gt; Residual standard error: 0.4965 on 270 degrees of freedom R&gt; Multiple R-squared: 0.8115, Adjusted R-squared: 0.8108 R&gt; F-statistic: 1162 on 1 and 270 DF, p-value: &lt; 2.2e-16 15.1.1 The intercept The intercept is the best estimate of the starting point of the fitted line on the lefthand side of the graph. You will notice that there is also an estimate for the standard error of the estimate for the intercept. 15.1.2 The regression coefficient The interpretation of the regression coefficient is simple. For every one unit of change in the independent variable (here waiting time) there is a corresponding change in the dependent variable (here the duration of the eruption). This is the slope or gradient, and it may be positive or negative. In the example the coefficient of determination of the line is denoted by the value 0.076 min.min-1 in the column termed Estimate and in the row called waiting (the latter name will of course depend on the name of the response column in your dataset). The coefficient of determination multiplies the response variable to produce a prediction of the response based on the slope of the relationship between the response and the predictor. It tells us how much one unit in change of the independent variable determines the corresponding change in the response variable. There is also a standard error for the estimate. 15.1.3 A graph of the linear regression slope &lt;- round(eruption.lm$coef[2], 3) # p.val &lt;- round(coefficients(summary(eruption.lm))[2, 4], 3) # it approx. 0, so... p.val = 0.001 r2 &lt;- round(summary(eruption.lm)$r.squared, 3) ggplot(data = faithful, aes(x = waiting, y = eruptions)) + geom_point() + annotate(&quot;text&quot;, x = 45, y = 5, label = paste0(&quot;slope == &quot;, slope, &quot;~(min/min)&quot;), parse = TRUE, hjust = 0) + annotate(&quot;text&quot;, x = 45, y = 4.75, label = paste0(&quot;italic(P) &lt; &quot;, p.val), parse = TRUE, hjust = 0) + annotate(&quot;text&quot;, x = 45, y = 4.5, label = paste0(&quot;italic(r)^2 == &quot;, r2), parse = TRUE, hjust = 0) + stat_smooth(method = &quot;lm&quot;, colour = &quot;salmon&quot;) + labs(title = &quot;Old Faithful eruption data&quot;, subtitle = &quot;Linear regression&quot;, x = &quot;Waiting time (minutes)&quot;, y = &quot;Eruption duration (minutes)&quot;) 15.1.4 Predicting from the linear model Knowing \\(\\alpha\\) and \\(\\beta\\) allows us to predict what the erruption duration will be for a certain amount of waiting. Since the slope of the line is positive we can expect that the longer the waiting time is between eruptions the longer the eruption would be. But how can we quantify this? We start by extracting the coefficients (both the intercept and the regression coefficient). Then we use these values to reassemble the regression equation that we have written out above (i.e., \\(eruption_{n}=\\beta \\cdot waiting_{n}+\\alpha+\\epsilon\\)). Here’s how: # use the accessor function to grab the coefficients: erupt.coef &lt;- coefficients(eruption.lm) erupt.coef R&gt; (Intercept) waiting R&gt; -1.87401599 0.07562795 # how long would an eruption last of we waited, say, 80 minutes? waiting &lt;- 80 # the first and second coef. can be accessed using the # square bracket notation: erupt.pred &lt;- erupt.coef[1] + (erupt.coef[2] * waiting) erupt.pred # the unit is minutes R&gt; (Intercept) R&gt; 4.17622 The prediction is that, given a waiting time of 80 minutes since the previous eruption, the next eruption will last 4.176 minutes. There is another way to do this. The predict() function takes a dataframe of values for which we want to predict the duration of the eruption and returns a vector with the waiting times: pred.val &lt;- data.frame(waiting = c(60, 80, 100)) predict(eruption.lm, pred.val) # returns waiting time in minutes R&gt; 1 2 3 R&gt; 2.663661 4.176220 5.688779 15.1.5 The coefficient of determination, \\(r^{2}\\) The coefficient of determination, the \\(r^{2}\\), of a linear model is the quotient of the variances of the fitted values, \\(\\hat{y_{i}}\\), and observed values, \\(y_{i}\\), of the dependent variable. If the mean of the dependent variable is \\(\\bar y\\), then the \\(r^{2}\\) is: \\[r^{2}=\\frac{\\sum(\\hat{y_{i}} - \\bar{y})^{2}}{\\sum(y_{i} - \\bar{y})^{2}}\\] In our Old Faithful example, the coefficient of determination is returned together with the summary of the eruption.lm object, but it may also be extracted as: summary(eruption.lm)$r.squared R&gt; [1] 0.8114608 What does the \\(r^{2}\\) tell us? It tells us the “fraction of variance explained by the model” (from the summary.lm() help file). In other words it is the proportion of variation in the dispersion (variance) of the measured dependent variable, \\(y\\), that can be predicted from the measured independent variable, \\(x\\) (or variables in the case of multiple regressions). It gives us an indication of how well the observed outcome variable is predicted by the observed influential variable, and in the case of a simple linear regression, the geometric relationship of \\(y\\) on \\(x\\) is a straight line. \\(r^{2}\\) can take values from 0 to 1: a value of 0 tells us that there is absolutely no relationship between the two, whilst a value of 1 shows that there is a perfect fit and a scatter of points to denote the \\(y\\) vs. \\(x\\) relationship will all fall perfectly on a stright line. n &lt;- 100 set.seed(666) rand.df &lt;- data.frame(x = seq(1:n), y = rnorm(n = n, mean = 20, sd = 3)) ggplot(data = rand.df, aes(x = x, y = y)) + geom_point(colour = &quot;blue&quot;) + stat_smooth(method = &quot;lm&quot;, colour = &quot;purple&quot;, size = 0.75, fill = &quot;turquoise&quot;, alpha = 0.3) + labs(title = &quot;Random normal data&quot;, subtitle = &quot;Linear regression&quot;, x = &quot;X (independent variable)&quot;, y = &quot;Y (dependent variable)&quot;) Regressions may take on any relationship, not only a linear one. For example, there are parabolic, hyperbolic, logistic, exponential, etc. relationships of \\(y\\) on \\(x\\), and here, too, does \\(r^{2}\\) tell us the same thing. If we assume that the samples were representatively drawn from a population (i.e. the sample fully captures the relationship of \\(y\\) on \\(x\\) that is present in the entire population), the \\(r^{2}\\) will represent the relationship in the population too. In the case of our Old Faithful data, the \\(r^{2}\\) is 0.811, meaning that the proportion of variance explained is 81.1%; the remaining 18.9% is not (yet) accounted for by the linear relationship. Adding more predictors into the regression (i.e. a multiple regression) might consume some of the unexplained variance and increase the overall \\(r^{2}\\). 15.1.6 Significance test for linear regression There are several hypothesis tests associated with a simple linear regression. All of them assume that the residual error, \\(\\epsilon\\), in the linear regression model is independent of \\(x\\) (i.e. nothing about the structure of the error term can be inferred based on a knowledge of \\(x\\)), is normally distributed, with zero mean and constant variance. We say the residuals are i.i.d. (independent and identically distributed, which is a fancy way of saying they are random). We can decide whether there is any significant relationship (slope) of \\(y\\) on \\(x\\) by testing the null hypothesis that \\(\\beta=0\\). Rejecting the null hypothesis causes the alternate hypothesis of \\(\\beta \\neq 0\\) to be accepted. This test is automatically performed when fitting a linear model in R and asking for a summary of the regression object, but it is insightful and important to know that the test is simply a one-sample t-test. In the regression summary the probability associated with this test is given in the Coefficients table in the column called Pr(&gt;|t|). In the Old Faithful data, the p-value associated with waiting is less than 0.05 and we therefore reject the null hypothesis that \\(\\beta=0\\). So, there is a significant linear relationship of eruption duration on the waiting time between eruptions. Question: Note that there is also a hypothesis test in the (Intercept) row. What does this do? 15.1.7 Confidence interval for linear regression Again we have to observe the assumption of i.i.d. as before. For a given value of \\(x\\), the 95% confidence interval around the mean of the observed dependent variable, \\(\\bar{y}\\), can be obtained as follows: pred.val &lt;- data.frame(waiting = c(80)) predict(eruption.lm, pred.val, interval = &quot;confidence&quot;) R&gt; fit lwr upr R&gt; 1 4.17622 4.104848 4.247592 So, the 95% confidence interval of the mean eruption duration for the waiting time of 80 minutes is between 4.105 and 4.248 minutes. 15.1.8 Prediction interval for linear regression Observe that \\(\\epsilon\\) is i.i.d. For a given value of \\(x\\), the interval estimate of the future dependent variable, \\(y\\), is called the prediction interval. The way we do this is similar to finding the confidence interval: pred.val &lt;- data.frame(waiting = c(80)) predict(eruption.lm, pred.val, interval = &quot;prediction&quot;) R&gt; fit lwr upr R&gt; 1 4.17622 3.196089 5.156351 The intervals are wider. The difference between confidence and prediction intervals is subtle and requires some philosophical consideration. In practice, if you use these intervals to make inferences about the population from which the samples were drawn, use the prediction intervals. If you instead want to describe the samples which you have taken, use the confidence intervals. 15.1.9 Residual plot 15.1.10 Standardised residual 15.1.11 Normal probability plot of residuals 15.2 Using an additional categorical variable When you use a categorical variable, in R the intercept represents the default position for a given value in the categorical column. Every other value then gets a modifier to the base prediction. "]
]
