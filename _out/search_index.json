[
["index.html", "Basic Statistics Preface", " Basic Statistics A primer in basic statistics for BCB (Hons) 2018 AJ Smit 2018-02-01 Preface This is a workshop about the practice of the basic statistics used by biologists, and not about the theory and mathematical underpinnings of the methods used. Each of the Chapters will cover a basic kind of statistical approach, and the main classes of data it applies to. Since much insight and understanding can be gained from visualising our data, we will also explore the main types of graphical summaries that best accompany the statistical methodologies. It is our intention to demonstrate how we go about analysing our data. "],
["prerequisites.html", "Prerequisites", " Prerequisites A prerequisite for this course is a basic proficiency in using R (R Core Team 2017). The necessary experience will have been gained from completing the Intro R Workshop: Data Manipulation, Analysis and Graphing Workshop that was part of your BCB Core Honours module (i.e. Biostatistics). You will also need a laptop with R and RStudio installed as per the instructions provided in that workshop. If you do not have a personal laptop, most computers in the 5th floor lab will be correctly set up for this purpose. References "],
["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction “A scientist worthy of a lab coat should be able to make original discoveries while wearing a clown suit, or give a lecture in a high squeaky voice from inhaling helium. It is written nowhere in the math of probability theory that one may have no fun.” — Eliezer Yudkowsky “Prediction is very difficult, especially about the future.” –– Niels Bohr "],
["preliminaries.html", "1.1 Preliminaries", " 1.1 Preliminaries "],
["venue-date-and-time.html", "1.2 Venue, date and time", " 1.2 Venue, date and time Quantitative Ecology is scheduled to replace Plant Ecophysiology, and will run between May 14th and June 29th, 2018. This workshop will take place from 9:00–16:00 on one day in each week during this period. There will also be a field component, where you will be taught about ecological field sampling in marine and terrestrial environments; this will also offer an opportunity to collect real data using actual ecological field methods (these will also be covered in the course), which we will then analyse using the multivariate methods used in this workshop. "],
["course-outline.html", "1.3 Course outline", " 1.3 Course outline Descriptive versus inferential statistics Measures of central tendency Measures of dispersion and variability Distributions Graphical data displays T-tests (one- and two-sample, etc.) Confidence intervals ANOVA (one- and two-way) Linear mixed models Testing assumptions; transformations Non-parametric tests (Mann-Whitney, Kruskal-Wallis) Chi-square tests Correlation Linear models (linear regression) Generalised linear models "],
["about-this-workshop.html", "1.4 About this Workshop", " 1.4 About this Workshop The aim of this five-day introductory workshop is to guide you through… "],
["this-is-biology-why-more-r-coding.html", "1.5 This is biology: why more R coding?", " 1.5 This is biology: why more R coding? Please refer to the Intro R Workshop: Data Manipulation, Analysis and Graphing for why we feel strongly that you use R (R Core Team 2017) for the analyses that we will perform here. All of the reasons provided there are valid here too, but one reason perhaps more so than others — R and RStudio promote the principles of reproducible research, and in fact make it very easy to implement. We will focus on some of these principles throughout the workshop, and the assignments will in fact require that you submit a fully functional working script, complete with all the notes, memos, examples, data, executable code, and output that will result from completing the course material. What other oprions are there for analysing the kinds of data that we will encounter in biological research? Software packages like the ones you may be familiar with, such as Statistica and SPSS, are often used to perform many of the analyses we will encounter. They are rather limited with regards to the full scope of modern statistical methods in use by biologists today, but many people still use these kinds of software as the provide the basic kinds analyses that still form the staple of the biological and medical sciewnces. For the many reasons provided above, we prefer to use R as the engine within which to do our biological data analysis. R is used by academic statisticians the world over, and it is therefore an excellent choice for our purpose here… References "],
["installing-r-and-rstudio.html", "1.6 Installing R and RStudio", " 1.6 Installing R and RStudio We assume that you already have R installed on your computer, as all of you will have already completed the the Intro R Workshop. If you need a refresher, please refer to Intro R Workshop: Data Manipulation, Analysis and Graphing for the installation instructions. "],
["resources.html", "1.7 Resources", " 1.7 Resources 1.7.1 Required reading 1.7.2 Resources about multivatiate ecological methods 1.7.3 General resources about R "],
["style-and-code-conventions.html", "1.8 Style and code conventions", " 1.8 Style and code conventions Early on, develop the habit of unambiguous and consistent style and formatting when writing your code, or anything else for that matter. Pay attention to detail and be pedantic. This will benefit your scientific writing in general. Although many R commands rely on precisely formatted statements (code blocks), style can nevertheless to some extent have a personal flavour to it. The key is consistency. In this book we use certain conventions to improve readability. We use a consistent set of conventions to refer to code, and in particular to typed commands and package names. Package names are shown in a bold font over a grey box, e.g. tidyr. Functions are shown in normal font followed by parentheses and also over a grey box , e.g. plot(), or summary(). Other R objects, such as data, function arguments or variable names are again in normal font over a grey box, but without parentheses, e.g. x and apples. Sometimes we might directly specify the package that contains the function by using two colons, e.g. dplyr::filter(). Commands entered onto the R command line (console) and the output that is returned will be shown in a code block, which is a light grey background with code font. The commands entered start at the beginning of a line and the output it produces is preceded by R&gt;, like so: rnorm(n = 10, mean = 0, sd = 13) R&gt; [1] 20.740313 33.228179 22.949770 21.776450 25.027786 10.426938 2.456074 R&gt; [8] -2.101046 7.080193 11.606892 Consult these resources for more about R code style : Google’s R style guide The tidyverse style guide Hadley Wickham’s advanced R style guide We can also insert maths expressions, like this \\(f(k) = {n \\choose k} p^{k} (1-p)^{n-k}\\) or this: \\[f(k) = {n \\choose k} p^{k} (1-p)^{n-k}\\] "],
["assessment-and-teaching-philosophy.html", "1.9 Assessment and teaching philosophy:", " 1.9 Assessment and teaching philosophy: Grades will be based on performance of two take home exams, and an individual project and homework problem sets. The exams and individual project will represent 30% of the grade. The homework problem sets will make up the remaining 10%. In cases where students are borderline between lower and higher grades, a high level of participation in the class discussions and class in general will win the day for the higher grade. Homework problems are essential to understanding of the materials. Although the homework comprises only 10% of the final grade, performance on the exams is usually correlated with effort on the homework problems. Whereas plagiarism will not be tolerated, students ARE encouraged to work together to learn from one another (especially those from the same IVN site) and solve problems in a collaborative and collegial way (aside from the take home exam). "],
["about-this-document.html", "1.10 About this document", " 1.10 About this document This document, which as available as a HTML file that’s viewable on a web browser of your choice (anything will do, but we discourage using Internet Explorer) and as a PDF (accessible from the link at the top of any of the website’s pages) that may be printed, was prepared by the software tools available to R via RStudio. We use the package called bookdown that may be accessed and read about here to produce this documentation. The entire source code to reproduce this book is available from my GitHub account. knitr::include_graphics(&quot;figures/bookdown_hex_logo.png&quot;) Figure 1.1: Bookdown hex. You will notice that this repository uses GitHub, and you are advised to set up your own repository for R scripts and all your data. We will touch on GitHub and the principles of reproducible research later, and GitHub forms a core ingredient of such a workflow. The R session information when compiling this book is shown below: sessionInfo() R&gt; R version 3.4.3 (2017-11-30) R&gt; Platform: x86_64-apple-darwin15.6.0 (64-bit) R&gt; Running under: macOS Sierra 10.12.6 R&gt; R&gt; Matrix products: default R&gt; BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib R&gt; LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib R&gt; R&gt; locale: R&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 R&gt; R&gt; attached base packages: R&gt; [1] stats graphics grDevices utils datasets base R&gt; R&gt; loaded via a namespace (and not attached): R&gt; [1] Rcpp_0.12.15 bookdown_0.6 png_0.1-7 digest_0.6.14 R&gt; [5] rprojroot_1.3-2 backports_1.1.2 magrittr_1.5 evaluate_0.10.1 R&gt; [9] highr_0.6 stringi_1.1.6 rmarkdown_1.8 tools_3.4.3 R&gt; [13] stringr_1.2.0 xfun_0.1 yaml_2.1.16 compiler_3.4.3 R&gt; [17] htmltools_0.3.6 knitr_1.18 methods_3.4.3 "],
["types-of-data.html", "Chapter 2 Types of data", " Chapter 2 Types of data “The plural of anecdote is not data.” — Roger Brinner In this chapter we will, firstly, look at the different kinds of biological and environmental data that are typically encountered by most biologists. The data seen here is not an exclusive list of all the various types of data out there, but it should represent the bulk of our needs. After we have become familiar with the different kinds of data, we will look at summaries of these data, which is generally required as the starting point for our analysis. After summarising the data in tables and so forth, we may want to produce graphical summaries to see broad patterns and trends; visual data representations, which complement the tabulated data, will be covered in a later chapter (Chapter 3). Both of these approaches form the basis of ‘exploratory data analysis.’ "],
["data-classes.html", "2.1 Data classes", " 2.1 Data classes In biology we will encounter many kinds of data, and depending on which kind, the type of statistical analysis will decided. 2.1.1 Numerical data Numerical data are quantitative in nature. They represent things that can be objectively counted or measured. 2.1.1.1 Nominal (discrete) data Integer data (discrete numbers or whole numbers), such as counts. For example, family A has 3 children and family B has 1 child. Integer data usually answer the question, “how many?” In R integer data are called int or &lt;int&gt;. 2.1.1.2 Continuous data These usually represent measured ‘things,’ such as something’s heat content (temperature, measured in degrees Celsius) or distance (measured in meters or similar), etc. They can be rational numbers including integers and fractions, but typically they have an infinite number of ‘steps’ that depends on rounding (they can even be rounded to whole integers) or considerations such as precision and accuracy. Often, continuous data have upper and lower bounds that depends on the characteristics of the phenomenon being studied or the measurement being taken. In R, continuous data are denoted num or &lt;dbl&gt;. 2.1.1.3 Dates Dates are a special class of continuous data, and there are many different replresenations of the date classes. This is a complex group of data, and we will not cover much of it here. 2.1.2 Qualitative data Qualitative data may be well-defined categories or they may be subjective, and generally include descriptive words for classes (e.g. mineral, animal , plant) or rankings (e.g. good, better, best). 2.1.2.1 Categorical data Also known as qualitative data. Because there are categories, the number of members belonging to each of the categories can be counted. For example, there are three red flowers, 66 purple flowers, and 13 red flowers. The categories cannot be ranked relative to each other; in the example just provided, for instance, no value judgement can be assigned to the different colours. It is not better to be red than it is to be purple. There are just fewer red flowers than purple ones. Contrast this to another kind of categortical data called ‘ordinal data’ (see next). This class of data in an R dataframe (or in a ‘tibble’) is indicated by Factor or &lt;fctr&gt;. 2.1.2.2 Ordinal data This is a type of categorical data where the classes are ordered (a synonymn is “ranked”), typically from low to high (or vice versa), but where the magnitude between the ordered classes cannot be precisely measured or quantified. In other words, the difference between them is somewhat subjective (i.e. it is qualitative rather than quantitative). These data are on an ordinal scale. The data may be entered as descriptive character strings (i.e. as words), or they may have been translated to an ordered vector of integers; for example, “1” for terrible, “2” for so-so, “3” for average, “4” for good and “5” for brilliant. Irrespective of how the data are present in the dataframe, computationally (for some calculations) they are treated as an ordered sequence of integers, but they are simultaneously treated as categories (say, where the number of responses that report “so-so” can be counted). Ordinal data usually answer questions such as, “how many categories can the phonomenon be divided into, and how does each category rank with respect to the others?” Columns containing this kind of data are named Ord.factor or &lt;ord&gt;. 2.1.3 Binary data Right or wrong? True or false? Accept or reject? Black or white? Positive or negative? Good or bad? You get the idea… In other words, these are observations or responses that can take only one of two mutually exclusive outcomes. In R these are treated as ‘Logical’ data that take the values of TRUE or FALSE (note the case), and they are indicated by logi or &lt;lgl&gt;. 2.1.4 Complex numbers 2.1.5 Character values 2.1.6 Missing values "],
["viewing-our-data.html", "2.2 Viewing our data", " 2.2 Viewing our data There are many ways of finding broad views of our data in R. The first few functions that we will look at were designed to simply scrutinise the contents of the tibbles, which is the ‘tidyverse’ name for the general ‘container’ that holds our data in the software’s environment (i.e. in a block of the computer’s memory dedicated to the R software). Whatever data are in R’s environment will be seen in the ‘Environment’ tab in the top right of RStudio’s four panes. 2.2.1 From the Environment pane The first way to see what’s in the tibble is not really a function at all, but a convenient (and lazy) way of quickly seeing a few basic things about our data. Let us look at the ChickWeight data. Load it like so (you’ll remember from the Intro R Workshop): # loads the tidyverse functions; it contains the &#39;as_tibble()&#39; function library(tidyverse) # the &#39;ChickWeight&#39; data are built into R; # here we assign it as a tibble to an object named &#39;chicks&#39; chicks &lt;- as_tibble(ChickWeight) In the Environment pane, the object named chicks will now appear under the panel named Data. To the left of it is a small white arrow in a blue circular background. By default the arrow points to the right. Clicking on it causes it to point down, which denotes that the data contained within the tibble have become expanded. The names of the columns (more correctly called ‘variables’) can now be seen. There you can see the variables weight, Time, Chick and Diet. The class of data they represent can be seen too: there’s continuous data of class num, a variable of Ord.factor, and a categorical variable of class Factor. Beneath these there’s a lot of attributes that denote some meta-data, which you may safely ignore for now. (#fig:chicks_1)What is in the Chicks data? (#fig:chicks_2)This is what is in the Chicks data 2.2.2 head() and tail() The head() and tail() functions simply display top and bottom portions of the tibble, and you may add the n argument and an integer to request that only a certain number of rows is returned; by default the top of bottom six rows are displayed. There are various bits of additional information printed out. The display will change somewhat if there are many more variables than that which can comfortably fit within the width of the window (typically the Console). The same kinds of information as was returned with the Environment pane expansion arrow are displayed, but the data class is now accompanied by an angle bracket (i.e. &lt;...&gt;) notation. For example, num in the Environment pane and &lt;dbl&gt; as per the head() or tail() methods are exactly the same: both denote continuous (or ‘double precision’) data. head(chicks) ## # A tibble: 6 x 4 ## weight Time Chick Diet ## &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;fct&gt; ## 1 42.0 0 1 1 ## 2 51.0 2.00 1 1 ## 3 59.0 4.00 1 1 ## 4 64.0 6.00 1 1 ## 5 76.0 8.00 1 1 ## 6 93.0 10.0 1 1 tail(chicks, n = 2) ## # A tibble: 2 x 4 ## weight Time Chick Diet ## &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;fct&gt; ## 1 264 20.0 50 4 ## 2 264 21.0 50 4 As an alternative to head(), you may also simply type the name of the object (here chicks) in the Console (or write it in the Source Editor if it is necessary to retain the function for future use) and the top portion of the tibble will be displayed, again trimmed to account for the width of the display. 2.2.3 colnames() This function simply returns a listing of the variable (column) names. colnames(chicks) ## [1] &quot;weight&quot; &quot;Time&quot; &quot;Chick&quot; &quot;Diet&quot; There is an equivalent function called rownames() that may be used to show the names of rows in your tibble, if these are present. Row names are generally discouraged, and we will refrain from using them here. 2.2.4 summary() The next way to see the contents of the tibble is to apply the summary() function. Here we see something else. Some descriptive statistics that describe properties of the full set of data are now visible. These summary statistics condense each of the variables into numbers that describe some properties of set of data within each column. You will already know the concepts of the ‘minimum,’ ‘median,’ ‘mean,’ and ‘maximum.’ These are displayed here. summary(chicks) ## weight Time Chick Diet ## Min. : 35.0 Min. : 0.00 13 : 12 1:220 ## 1st Qu.: 63.0 1st Qu.: 4.00 9 : 12 2:120 ## Median :103.0 Median :10.00 20 : 12 3:120 ## Mean :121.8 Mean :10.72 10 : 12 4:118 ## 3rd Qu.:163.8 3rd Qu.:16.00 17 : 12 ## Max. :373.0 Max. :21.00 19 : 12 ## (Other):506 This will serve well as an introduction to the next chapter, which is about the descriptive stastics. What are they, and how do we calculate them? "],
["descriptive-statistics.html", "Chapter 3 Descriptive statistics", " Chapter 3 Descriptive statistics “I think it is much more interesting to live with uncertainty than to live with answers that might be wrong.” –– Richard Feynman Talk about inferential and descriptive statistics, and the distinction between the two… Decriptive statistics and the associated statistical and graphical data summaries will be covered in Chapters 3 (this one) and 5. In Chapter 4 we will introduce the concepts of data distributions, knowledge of which is required to select the most appropriate inferential statistical methods. So, the materials covered in the next three chapters concern a broad discussion that will aid us in understanding our data better prior to analysing it. In this work flow it emerges that descriptive statistics generally precedes the inferential statistics. Let us now turn to some of the most commonly used discriptive statistics, and learn about how to calculate them. "],
["samples-and-populations.html", "3.1 Samples and populations", " 3.1 Samples and populations This is a simple toy example. In real life, however, our data will be available in a tibble (initially perhaps captured in MS Excel before importing it as a .csv file into R, where the tibble is created). To see how this can be done more realistically using actual data, let us turn to the ChickenWeight data, which, as before, place in the object chicks. Recall the pipe operator (%&gt;%, pronounced ‘then’) that we introduced in the Intro R Workshop — we will use that here, throughout. Let us calculate the sample size: To determine the sample size we can use the length() or n() functions; the latter is for use within dplyr’s summarise() method, and it is applied without writing anything inside of the (), like this: # first load the tidyverse packages that has the pipe operator, %&gt;% library(tidyverse) chicks &lt;- as_tibble(ChickWeight) # how many weights are available across all Diets and Times? chicks %&gt;% summarise(length = n()) ## # A tibble: 1 x 1 ## length ## &lt;int&gt; ## 1 578 # the same as length(chicks$weight) ## [1] 578 "],
["measures-of-central-tendency.html", "3.2 Measures of central tendency", " 3.2 Measures of central tendency Measures of central tendency. Statistic Function Mean mean() Median median() The measures of central tendency are also sometimes called ‘location’ statistics. We have already seen summaries of the mean and the median when we called to summary() function on the chicks data in Chapter 2. Here we shall show you how they can be calculated using some built-in R functions. 3.2.1 The mean The sample mean is the arithmetic average of the data, and it is calculated by summing all the data and dividing it by the sample size, n. The mean, \\(\\bar{x}\\), is calculated thus: \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i} = \\frac{x_{1} + x_{2} + \\cdots + x_{n}}{n}\\] where \\(x_{1} + x_{2} + \\cdots + x_{n}\\) are the observations and \\(n\\) is the number of observations in the sample. In R one can quickly apply the mean() function to some data. Let us create a vector of arbitrary numbers using the ‘combine’ function, c(), and then apply the function for the mean: # combine a series of numbers into a vector; # hint: use this function in the exercises that we will require from you # later on... dat1 &lt;- c(23, 45, 23, 66, 13) mean(dat1) ## [1] 34 chicks %&gt;% summarise(mean_wt = mean(weight)) ## # A tibble: 1 x 1 ## mean_wt ## &lt;dbl&gt; ## 1 122 Above, we use another tidyverse package, dplyr and its summarise() function, whose purpose it is to summarise the entire column into one summary statistic, in this case the mean. We can achieve the same using the more traditional syntax, which in some instances may be slightly less verbose, but less user-friendly, especially when multiple summary statistics are required (we shall later on how we can summarise a vector of data into multiple statistics). The traditional syntax is: # the &#39;$&#39; operator is used to denote a variable inside of the tibble mean(chicks$weight) ## [1] 121.8183 Notice above how the two approaches display the result differently: in the first instance, using summarise(), the answer is rounded to zero decimal places; in the second, it is displayed (here) at full precision. The precision of the answer that you require depends on the context of your study, so make sure that you use the appropriate number of significant digits. Using the summarise() approach again, here is how you can adjust the number of decimal places of the answer: # the value printed in the HTML/PDF versions is incorrect; # check in the console for correct output chicks %&gt;% summarise(mean_wt = round(mean(weight), 1)) ## # A tibble: 1 x 1 ## mean_wt ## &lt;dbl&gt; ## 1 122 Question: What happens when there are missing values (NA)? Consult the help file for the mean() function, discuss amongst yourselves, and then provide a demonstration to the class of how you would handle missing values. Hint: use the c() function to capture a series of data that you can then use to demonstrate your understanding. At this point it might be useful to point that the mean (or any function for that matter, even one that does not yet exist) can be programatically calculated. Let us demonstrate the principle using the mean: chicks %&gt;% summarise(mean_wt = sum(weight) / n()) ## # A tibble: 1 x 1 ## mean_wt ## &lt;dbl&gt; ## 1 122 The mean is quite sensitive to the presence of outliers or extreme values in the data, and it is advised that its use be reserved for normally distributed data from which the extremes/outliers have been removed. When extreme values are indeed part of our data and not simply ‘noise,’ then we have to resort to a different measure of central tendency: the median. 3.2.2 The median The median can be calculated by ‘hand’ (if you have a small enough amount of data) by arranging all the numbers in sequence from low to high, and then finding the middle value. If there are five numbers, say 5, 2, 6, 13, 1, then you would arrange them from low to high, i.e. 1, 2, 5, 6, 13. The middle number is 5. This is the median. But there is no middle if we have an even number of values. What now? Take this example sequence of six integers (they may also be floating point numbers), which has already been ordered for your pleasure: 1, 2, 5, 6, 9, 13. Find the middle two numbers (i.e. 5, 6) and take the mean. It is 5.5. That is the median. Let us find the median for the weights of the chickens in the ChickWeight data: chicks %&gt;% summarise(med_wt = median(weight)) ## # A tibble: 1 x 1 ## med_wt ## &lt;dbl&gt; ## 1 103 The median is therefore the value that separates the lower half of the sample data from the higher half. In normally distributed continuous data the median is equal to the mean. Comparable concepts to the median are the 1st and 3rd quartiles, which, respectively, separate the first quarter of the data from the last quarter — see later. The advantage of the median over the mean is that it is unaffected (i.e. not skewed) by extreme values or outliers, and it gives an idea of the typical value of the sample. The median is also used to provide a robust description of non-parametric data (see Chapter X for a discussion on normal data and other data distributions). "],
["measures-of-variation-and-spread.html", "3.3 Measures of variation and spread", " 3.3 Measures of variation and spread Since the mean or median does not tell us everything there is to know about data, we will also have to determine some statistics that inform us about the variation (or spread) around the central/mean value. Measures of variation and spread. Statistic Function Variance var() Standard deviation median() Minimum min() Maximum max() Range range() 3.3.1 The variance and standard deviation The sample variance, \\(S^{2}\\), may be calculated according to the following formula: \\[S^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}\\] This reads: “the sum of the squared differences from the mean, divided by the sample size minus 1.” To get the standard deviation, \\(S\\), we take the square root of the variance, i.e. \\(S = \\sqrt{S^{2}}\\). No need to plug these equations into MS Excel. Let us quickly calculate \\(S\\) in R. Again, we use the chicks data: chicks %&gt;% summarise(sd_wt = sd(weight)) ## # A tibble: 1 x 1 ## sd_wt ## &lt;dbl&gt; ## 1 71.1 The interpretation of the concepts mean and median is fairly straight forward and intuitive. Not so for the measures of variance. What does \\(S\\) represent? Firstly, the unit of measurement of \\(S\\) is the same as that of \\(\\bar{x}\\) (but the variance doesn’t share this characteric). If temperature is measured in °C, then \\(S\\) also takes a unit of °C. Since \\(S\\) measures the dispersion around the mean, we write it as \\(\\bar{x} \\pm S\\) (note that often the mean and standard deviation are written with the letters mu and sigma, respectively; i.e. \\(\\mu \\pm \\sigma\\)). The smaller \\(S\\) the closer the sample data are to \\(\\bar{x}\\), and the larger the value is the further away they will spread out from \\(\\bar{x}\\). So, it tells us about the proportion of observations above and below \\(\\bar{x}\\). But what proportion? We invoke the the 68-95-99.7 rule: ~68% of the population (as represented by a random sample of \\(n\\) observations taken from the population) falls within 1\\(S\\) of \\(\\bar{x}\\) (i.e. ~34% below \\(\\bar{x}\\) and ~34% above \\(\\bar{x}\\)); ~95% of the population falls within 2\\(S\\); and ~99.7% falls within 3\\(S\\). knitr::include_graphics(&quot;figures/Standard_deviation_diagram.svg&quot;) Figure 1.1: The proportions of data representation by the standard deviation. Credit: Wikipedia Like the mean, \\(S\\) is affected by extreme values and outliers, so before we attach \\(S\\) as a summary statistic to describe some data, we need to ensure that the data are in fact normally distributed. We will talk about how to do this in a later section (Section X), where we will go over the numerous ways to check the assumption of normality. When the data are found to be non-normal, we need to find appropriate ways to express the spread of the data. Enter the quartiles. 3.3.2 Quantiles A more forgiving approach (forgiving of the extremes, often called ‘robust’) is to divide the distribution of ordered data into quarters, and find the points below which 25% (0.25, the first quartile), 50% (0.50, the median) and 75% (0.75, the third quartile) of the data are distributed. These are called quartiles (for ‘quarter;’ not to be confused with quantile, which is a more general form of the function that can be used to divide the distribution into any arbitrary proportion from 0 to 1). In R we use the quantile() function to provide the quartiles; we demonstrate two approaches: quantile(chicks$weight) ## 0% 25% 50% 75% 100% ## 35.00 63.00 103.00 163.75 373.00 wt_summary &lt;- chicks %&gt;% summarise(min_wt = min(weight), qrt1_wt = quantile(weight, p = 0.25), med_wt = median(weight), qrt3_wt = median(weight, p = 0.75), max_wt = max(weight)) # note median(weight) is the same as quantile(weight, p = 0.5) # in the summarise() implementation, above wt_summary ## # A tibble: 1 x 5 ## min_wt qrt1_wt med_wt qrt3_wt max_wt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 35.0 63.0 103 103 373 Question: What is different about the quantile() function that caused us to specify the calculation in the way in we we have done so, above? You will have to consult the help file, read it, understand it, think about it, and experiment with the ideas. Take 15 minutes to figure it out and report back to the class. 3.3.3 The minimum, maximum and range A description of the extent of the data can also be provided by the functions min(), max() and range(). These statistics apply to data of any distribution, and not only to normal data. This if often the first place you want to start when looking at the data for the first time. "],
["missing-values-1.html", "3.4 Missing values", " 3.4 Missing values Put some text here. "],
["descriptive-statistics-by-group.html", "3.5 Descriptive statistics by group", " 3.5 Descriptive statistics by group Above we have revised the basic kinds of summary statistics, and how to calculate them. This is nice. But it can be more useful. The real reason why we might want to see the desriptive statistics is to facilitate comparisons between groups. In the chicks data we calculated the mean (etc.) for all the chickens, over all the diet groups to which they had been assigned (there are four factors, i.e. Diets 1 to 4), and over the entire duration of the experiment (the experiment lasted 21 days). It would be more useful to see what the weights are of the chickens in each of the four groups at the end of the experiment — we can compare means (± SD) and medians (± interquartile ranges, etc.), for instance. You’ll notice now how the measures of central tendency is being combined with the measures of variability/range. Further, we can augment this statistical summary with many kinds of graphical summaries, which will be far more revealing of differences (if any) amongst groups. We will revise how to produce the group statistics and show a range of the graphical displays. 3.5.1 Groupwise summary statistics At this point you need to refer to Chapter 10 (Tidy data) and Chapter 11 (Tidier data) in the Intro R Workshop to remind yourself about in what format the data need to be before we can efficiently work with it. A hint: one observation in a row, and one variable per column. From this point, it is trivial to do the various data descriptions, visualisations, and analyses. Thankfully, the chicks data are already in this format. So, what are the summary statistics for the chickens for each diet group at day 21? grp_stat &lt;- chicks %&gt;% filter(Time == 21) %&gt;% group_by(Diet, Time) %&gt;% summarise(mean_wt = round(mean(weight, na.rm = TRUE), 2), sd_wt = round(sd(weight, na.rm = TRUE), 2), med_wt = median(weight, na.rm = TRUE), min_wt = min(weight), qrt1_wt = quantile(weight, p = 0.25), med_wt = median(weight), qrt3_wt = median(weight, p = 0.75), max_wt = max(weight), n_wt = n()) grp_stat ## # A tibble: 4 x 10 ## # Groups: Diet [?] ## Diet Time mean_wt sd_wt med_wt min_wt qrt1_wt qrt3_wt max_wt n_wt ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 21.0 178 58.7 166 96.0 138 166 305 16 ## 2 2 21.0 215 78.1 212 74.0 169 212 331 10 ## 3 3 21.0 270 71.6 281 147 229 281 373 10 ## 4 4 21.0 239 43.4 237 196 204 237 322 9 3.5.2 Displays of group summaries There are several kinds of graphical displays for your data. We will show some which are able to display the spread of the raw data, the mean or median, as well as the appropriate accompanying indicators of variation around the mean or median. library(ggplot2) library(ggpubr) # needed for arranging multi-panel plots ## Loading required package: magrittr ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract plt1 &lt;- chicks %&gt;% filter(Time == 21) %&gt;% ggplot(aes(x = Diet, y = weight)) + geom_point(data = grp_stat, aes(x = Diet, y = mean_wt), col = &quot;black&quot;, fill = &quot;red&quot;, shape = 23, size = 3) + geom_jitter(width = 0.05) + # geom_point() if jitter not required labs(y = &quot;Chicken mass (g)&quot;) + theme_pubr() plt2 &lt;- ggplot(data = grp_stat, aes(x = Diet, y = mean_wt)) + geom_bar(position = position_dodge(), stat = &quot;identity&quot;, col = NA, fill = &quot;salmon&quot;) + geom_errorbar(aes(ymin = mean_wt - sd_wt, ymax = mean_wt + sd_wt), width = .2) + labs(y = &quot;Chicken mass (g)&quot;) + theme_pubr() # position_dodge() places bars side-by-side # stat = &quot;identity&quot; prevents the default count from being plotted plt3 &lt;- chicks %&gt;% filter(Time == 21) %&gt;% ggplot(aes(x = Diet, y = weight)) + geom_boxplot(fill = &quot;salmon&quot;) + geom_jitter(width = 0.05, fill = &quot;white&quot;, col = &quot;blue&quot;, shape = 21) + labs(y = &quot;Chicken mass (g)&quot;) + theme_pubr() plt4 &lt;- chicks %&gt;% filter(Time %in% c(10, 21)) %&gt;% ggplot(aes(x = Diet, y = weight, fill = as.factor(Time))) + geom_boxplot() + geom_jitter(shape = 21, width = 0.1) + labs(y = &quot;Chicken mass (g)&quot;, fill = &quot;Time&quot;) + theme_pubr() ggarrange(plt1, plt2, plt3, plt4, ncol = 2, nrow = 2, labels = &quot;AUTO&quot;) Figure 3.1: A) Scatterplot of the mean and raw chicken mass values. B) Bar graph of the chicken mass values, showing ‘whiskers’ indicating ±1 SD. C) Box and whisker plot of the chicken mass data. Please see the help file for geom_boxplot() for what the graph components mean. "],
["exercises.html", "3.6 Exercises", " 3.6 Exercises 3.6.1 Exercise 1 Notice how the data summary for chicken weights contained within wt_summary is very similar to the summary returned for weight when we apply summary(chicks). Please use the summarise() approach and construct a data summary with exactly the same summary statistics for weight as which summary() returns. "]
]
