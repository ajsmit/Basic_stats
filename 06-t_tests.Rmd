# Inferences about one or two populations


```{r prelim-opts, echo=FALSE}
knitr::opts_chunk$set(
  comment = "R>", 
  warning = FALSE, 
  message = FALSE
)
```

```{r}
library(tidyverse)
library(plotly)
```

## Assumptions

At the heart of many basic scientific inquiries is the simple question "Is A different from B?". The scientific notation for this question is:

> H0: Group A is not different from group B.
>
> H1: Group A is different from group B.

To answer this fundamental question one often uses a t-test. This test however requires us to make a couple of important assumptions that are _not_ guaranteed to be true. In fact, these assumptions are often violated because real data, especially biological data, are messy. In order to use a t-test to determine if a significant difference between two samples sets of data exists we must first establish that the data are normally distributed and that the data are homoscedastic. 

### Normal distributions

Remember from Chapter 5 what a normal distribution is/looks like? Let's have a peek below to remind ourselves:

```{r, fig.cap="Interactive histogram showing two randomly generated normal distributions."}
# Random normal data
r_dat <- data.frame(dat = c(rnorm(n = 1000, mean = 10, sd = 3), 
                             rnorm(n = 1000, mean = 8, sd = 2)),
                    sample = c(rep("1", 1000), rep("2", 1000)))

# Create histogram
h <- ggplot(data = r_dat, aes(x = dat, fill = sample)) +
  geom_histogram(position = "dodge", binwidth = 1, alpha = 0.8) +
  geom_density(aes(y = 1*..count.., fill = sample), colour = NA, alpha = 0.4) +
  labs(x = "value")

# Make it interactive
ggplotly(h)
```

Whereas histograms may be a pretty way to check the normality of our data, there is actually a statistical test for this, which is preferable to a visual inspection alone. But remember that you should _always_ visualise your data before performing any statistics on them. TO check the normality of the data we use the Shapiro-Wilk test. This test produces a W value and a _p_-value. We are only interested in the _p_-value as this is how we are to determine the normality of the data. If the _p_-value is __above__ 0.05 we may assume the data to be normally distributed. In order to demonstrate what the output of `shapiro.test()` looks like we will run it on all of the random data we generated.

```{r}
shapiro.test(r_dat$dat)
```

Note that this shows that the data are _not_ normally distributed. This is because we have incorrectly run this function on two different samples of data. To perform this test correctly, and in the tidy way, we need to select only the second piece of information from the `shapiro.test()` output and ensure that it is presented as a numeric value:

```{r}
r_dat %>% 
  group_by(sample) %>% 
  summarise(norm_dat = as.numeric(shapiro.test(dat)[2]))
```

Now we see that our two sample sets are indeed normally distributed.

### Homoscedasticity

Besides requiring that our data are normally distributed, we must also ensured that they are homoscedastic. This word means that the scedasticity (variance) of things are homogeneous () similar). In practical terms this means that the variance of the samples we are comparing should not be more than four times greater than one another. In R, we use the function `var()` to check the variance in a sample:

```{r}
r_dat %>% 
  group_by(sample) %>% 
  summarise(sample_var = var(dat))
```

Above we see that the variance of our two samples are homoscedastic because the variance of one is not more than four times greater than the other.

### Two for one

Because these two assumptions of normality and homoscedasticty are performed in tandem with one another, it is helpful to have a function that checks for both simultaneously. Below we see how just such a function would be written:

```{r}
two_assum <- function(x){
  x_var <- var(x)
  x_norm <- as.numeric(shapiro.test(x)[2])
  result <- c(x_var, x_norm)
  return(result)
}
```

To use our new function in a tidy way we use the following code:

```{r}
r_dat %>% 
  group_by(sample) %>% 
  summarise(sample_var = two_assum(dat)[1],
            sample_norm = two_assum(dat)[2])
```

Do these data meet our assumptions? How do we know this?

Once we have tested our assumptions we may perform a t-test to ascertain whether or not our samples are significantly different from one another. The base R function for this is `t.test()` however, by utilising the __`ggpubr`__ package we gain access to `compare_means()`, which allows us to perform any sort of test that compares sample sets of data. We will see throughout this and the following chapters why this is so useful.

```{r}
library(ggpubr)
```

## One-sample t-tests

Generally when we use a t-test it will be a two-sample t-test (see below). Occasionally however we may have only one sample set of data that we wish to compare against a known population mean, which is generally denoted as mu:

```{r}
# Create a single sample of random normal data
r_one <- data.frame(dat = rnorm(n = 20, mean = 20, sd = 5),
                    sample = "1")

# Compare random data against a population mean of 20
t.test(r_one$dat, mu = 20)

# Compare random data against a population mean of 30
t.test(r_one$dat, mu = 30)
```

What do the results of these two different tests show? Let's visualise these data to get a better understanding.

```{r, fig.cap="Boxplot of random normal data with. A hypothetical population mean of 20 is shown as a blue line, with the red line showing a mean of 30."}
ggplot(data = r_one, aes(y = dat, x = sample)) +
  geom_boxplot(fill = "lightsalmon") +
  # Population  mean (mu) = 20
  geom_hline(yintercept = 20, colour = "blue", 
             size = 3, linetype = "dashed") +
  # Population  mean (mu) = 30
    geom_hline(yintercept = 30, colour = "red", 
             size = 3, linetype = "dashed") +
  labs(y = "value") +
  coord_flip()
```

The boxplot above shows the distribution of our random data against two potential population means. Does this help now to illustrate the results of our one-sample t-tests?

### One-sided one-sample t-tests

As we may remember from Chapter 5, a distribution has two tails. When we are testing for significance we are generally looking for a result that lays in the far end of either of these tails. Occasionally, however, we may want to know if the result lays specifically in one of the tails. Explicitly the leading or trailing tail. To specify this in R we must add an argument as seen below:

```{r}
# Check against the trailing tail
t.test(r_one$dat, mu = 30, alternative = "less")

# Check against the leading tail
t.test(r_one$dat, mu = 30, alternative = "greater")
```

Are these the results we would have expected? Why does the second test not return a significant result?

### Two-sided one-sample t-tests

In R, the default setting for any comparison of means test is that it is two-sided so we do not need to state this explicitly. For the sake of thoroughness let's see how to do this below. Note that the results for the two following tests are identical:

```{r}
# R assumes we want a to-sided test
t.test(r_one$dat, mu = 30)

# But we can be explicit as we choose
t.test(r_one$dat, mu = 30, alternative = "two.sided")
```

## Two-sample t-tests

A two-sample t-test is used when we have two different samples that we would like to compare against one another. This is the most common use of this test. Let's generate some new random normal data and see how to use `compare_means()` to perform a t-test:

```{r}
# Random normal data
r_two <- data.frame(dat = c(rnorm(n = 20, mean = 4, sd = 1), 
                             rnorm(n = 20, mean = 5, sd = 1)),
                    sample = c(rep("1", 20), rep("2", 20)))

# Perform t-test
compare_means(dat ~ sample, data = r_two, method = "t.test")
```

Note above that in order to tell `compare_means()` to perform a t-test we feed it the argument `method = "t.test"`. The first argument we see in `compare_means()` is `dat ~ sample`. Usually in R when we see a `~` (tilde) we are creating what is known as a formula. A formula tells R how it should look for interactions between data and factors. For example `Y ~ X`. reads: Y as a factor of X. In our code above we see `dat ~ sample`. This means we are telling R that the t-test we want it to perform is when the `dat` column is a factor of the `sample` column. In plain English we are dividing up the `dat` column into the two different samples we have, and then running a t-test on these samples.

### One-sided two-sample t-tests

Just as with the one-sample t-tests above, we may also specify which tail of the distribution we are interested in when we compare the means of our two samples. We do so by providing the same arguments as previously:

```{r}
# Is sample 2 smaller than sample 1?
compare_means(dat ~ sample, data = r_two, method = "t.test", alternative = "less")

# Is sample 2 greater than sample 1?
compare_means(dat ~ sample, data = r_two, method = "t.test", alternative = "greater")
```

What do these results show? Is this surprising?

### Two-sided two-sample t-tests

Again, as stated above, the default setting in R for comparisons of means is that the test is two-sided. If one wants to state this explicitly it may be done as previously. Note that the results are identical.

```{r}
# Default settings
compare_means(dat ~ sample, data = r_two, method = "t.test")

# Explicitly state a two-sided test
compare_means(dat ~ sample, data = r_two, method = "t.test", alternative = "two.sided")
```

## Paired t-tests

Generally when one is comparing the means of two samples the order in which the individual records were recorded is not importing to the hypothesis. If for some reason they are, we must use a paired t-test. To do so we use the same structure as previously except we must now add one additional argument.

```{r}
compare_means(dat ~ sample, data = r_two, method = "t.test", paired = TRUE)
```

## Comparison of Two Population Proportions

All of the tests we covered above are designed to deal with continuous data, such as fish lengths or chlorophyll content. If we want to compare proportions (probabilities of success) of different samples against each other, or some known population mean, we need `prop.test()`. Let's create a dummy dataset to get a better idea of how this function works. Below we create some data showing the result of placing two different subjects, Jack and Jill, in separate sealed rooms for two hours (120 minutes). Once every minute a mosquito is let into the room before being extracted again. The columns `yes` and `no` show if the mosquito bit the subject during that one minute. Who says science can't be fun!

```{r}
mosquito <- matrix(c(70, 85, 50, 35), ncol = 2)
colnames(mosquito) <- c("yes", "no")
rownames(mosquito) <- c("Jack", "Jill")
mosquito
```

### One-sample and two-sample tests

As with t-tests, proportion tests may also be based on one sample, or two. If we have only one sample we must specify the total number of trials as well as what the expected population probability of success is. Because these are individual values, and not matrices, we will show what this would look like without using any objects but will rather give each argument within `prop.test()` an exact value. In the arguments within `prop.test()`, `x` denotes the number of successes recorded, `n` shows the total number of individual trials performed, and `p` is the expected probability. It is easiest to consider this as though it were a series of 100 coin tosses.

```{r}
# When the probability matches the population
prop.test(x = 45, n = 100, p = 0.5)

# When it doesn't
prop.test(x = 33, n = 100, p = 0.5)
```

If we have two samples that we would like to compare against one another we enter them into the function as follows:

```{r}
# NB: Note that the `mosquito` data are a matrix, NOT a data.frame
prop.test(mosquito)
```

Do mosquito's bite Jack and Jill at different proportions?

### One-sided and two-sided tests

AS with all other tests that compare values, proportion tests may be specified as either one or two-sided. Just to be clear, the default setting for `prop.test()`, like everything else, is a two-sided test. See code below to confirm that the results are identical with or without the added argument:

```{r}
# Default
prop.test(mosquito)

# Explicitly state two-sided test
prop.test(mosquito, alternative = "two.sided")
```

Should we want to specify only one of the tails to be considered, we do so precisely the same as with t-tests. Below are examples of what this code would look like:

```{r}
# Jack is bit less than Jill
prop.test(mosquito, alternative = "less")

# Jack is bit more than Jill
prop.test(mosquito, alternative = "greater")
```

Do these results differ from the two-sided test? What is different?

## Exercises

### Exercise 1

Find or create your own normally distributed data and think of a hypothesis you could use a t-test for. Write out the hypothesis, test it, and write a one sentence conclusion for it. Provide all of the code used to accomplish this.

### Exercise 1

Do the same as Exercise 1, but for probability data.
